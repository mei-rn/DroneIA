{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBX0zw6KLQGY"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6WTp0fe_6KhH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsm1FtoqLXmj"
      },
      "source": [
        "## Defining constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fy8GkK8y6suf"
      },
      "outputs": [],
      "source": [
        "STATE_SIZE = (10,10)\n",
        "ACTION_SIZE = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78prn4sdLunY"
      },
      "source": [
        "## Defining loading and saving of files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zmz1j9eb7gA7"
      },
      "outputs": [],
      "source": [
        "def load(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NqD37L7z7lC0"
      },
      "outputs": [],
      "source": [
        "def save(filename, Q_table):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(Q_table, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAxlCYl7LcDi"
      },
      "source": [
        "## Defining the static environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "TS8ylwsB6w8c"
      },
      "outputs": [],
      "source": [
        "class DroneGrid():\n",
        "\n",
        "    def __init__(self, grid):\n",
        "\n",
        "        self.grid = grid\n",
        "        self.empty_grid = grid\n",
        "        self.grid_size = np.array(grid).shape\n",
        "        self.observation_space = (self.grid_size[0]), (self.grid_size[1])\n",
        "        self.action_space = [0, 1, 2, 3] # 4 discrete actions: 0 = up, 1 = down, 2 = left, 3 = right\n",
        "        self.start_pos = (0, 0)  # Starting position at top left corner\n",
        "        self.goal_pos = (self.grid_size[0] - 1, self.grid_size[1] - 1)  # Goal position at bottom right corner\n",
        "        self.current_pos = self.start_pos  # Initialize current position\n",
        "        self.grid[self.current_pos[1]][self.current_pos[0]] = 3\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_pos = self.start_pos  # Reset current position to start position\n",
        "        self.grid = self.empty_grid\n",
        "        return self.current_pos, self.grid  # Return initial state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "kjRZ2jnusiED"
      },
      "outputs": [],
      "source": [
        "class QLEnvironment(DroneGrid):\n",
        "    def __init__(self, grid):\n",
        "        super().__init__(grid)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        assert action in self.action_space, f\"Invalid action {action}\"  # Check if action is valid\n",
        "\n",
        "        # Define movement based on action\n",
        "        if action == 0:  # Up\n",
        "            new_pos = (self.current_pos[0], self.current_pos[1] - 1)\n",
        "        elif action == 1:  # Down\n",
        "            new_pos = (self.current_pos[0], self.current_pos[1] + 1)\n",
        "        elif action == 2:  # Left\n",
        "            new_pos = (self.current_pos[0] - 1, self.current_pos[1])\n",
        "        elif action == 3:  # Right\n",
        "            new_pos = (self.current_pos[0] + 1, self.current_pos[1])\n",
        "\n",
        "        # Check if new position is within bounds and not an obstacle\n",
        "        if 0 <= new_pos[0] < self.grid_size[0] and 0 <= new_pos[1] < self.grid_size[1] and self.grid[new_pos[1]][new_pos[0]] != 1:\n",
        "\n",
        "            self.current_pos = new_pos  # Update current position\n",
        "            self.grid = self.empty_grid # Erase previous position of the drone\n",
        "\n",
        "            # Check if goal state is reached\n",
        "            done = (self.current_pos == self.goal_pos)\n",
        "\n",
        "            # Calculate reward\n",
        "            if done:\n",
        "                reward = 1.0  # Positive reward for reaching the goal\n",
        "\n",
        "            else:\n",
        "                reward = 0 #Negative reward for non-goal state\n",
        "                self.grid[new_pos[1]][new_pos[0]] = 3 # Update new position of the drone\n",
        "\n",
        "\n",
        "        elif 0 <= new_pos[0] < self.grid_size[0] and 0 <= new_pos[1] < self.grid_size[1] and self.grid[new_pos[1]][new_pos[0]] == 1:\n",
        "                done = False\n",
        "                reward = -0.1 # Negative reward for going in a wall\n",
        "\n",
        "\n",
        "        else:\n",
        "            done = False\n",
        "            reward = 0  # Negative reward for going out of bounds\n",
        "\n",
        "        return self.current_pos, self.grid, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INRXclRKfUL_"
      },
      "source": [
        "Function to compute an action in function of the epsilon-greedy algorith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jggC9KIOffdR"
      },
      "outputs": [],
      "source": [
        "def compute_action(current_state, Q_table, epsilon, environment):\n",
        "\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "        return np.random.choice(range(len(environment.action_space)))\n",
        "\n",
        "    else:\n",
        "        return np.argmax(Q_table[current_state])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQ9r7NkL3lQ"
      },
      "source": [
        "Loading a personalized map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JQF9q-v-AQI",
        "outputId": "e192b5b0-d960-4cbf-a671-264524616c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 1, 0, 0, 1, 1, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 1, 1], [0, 1, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "map_simple = load('map_simple.pkl')\n",
        "print(map_simple)\n",
        "\n",
        "map_mid = load('map_mid.pkl')\n",
        "print(map_mid)\n",
        "\n",
        "map_hard = load('map_hard.pkl')\n",
        "print(map_hard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUWCssuaL9Ck"
      },
      "source": [
        "Creating an instance of the environment through the loaded map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRYMCmkS-eqB",
        "outputId": "4aa09a5a-a3f3-4034-81de-b71d7ea1e8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 10)\n",
            "[0, 1, 2, 3]\n",
            "(10, 10)\n",
            "[0, 1, 2, 3]\n",
            "(10, 10)\n",
            "[0, 1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "environment_simple = QLEnvironment(map_simple)\n",
        "print(environment_simple.observation_space)\n",
        "print(environment_simple.action_space)\n",
        "\n",
        "environment_mid = QLEnvironment(map_mid)\n",
        "print(environment_mid.observation_space)\n",
        "print(environment_mid.action_space)\n",
        "\n",
        "environment_hard = QLEnvironment(map_hard)\n",
        "print(environment_hard.observation_space)\n",
        "print(environment_hard.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha7rhcl9BUpa"
      },
      "source": [
        "---\n",
        "# Q-Learning\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "513kKA3J7CLh"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, alpha=1, gamma=0.99,  epsilon=0.99, epsilon_decay=0.00025, episodes = 10001, max_iter_episode = 500):\n",
        "    start_time = time.time()\n",
        "    Q = np.zeros((env.grid_size[0]*env.grid_size[1], len(env.action_space)), dtype=np.float32) #Initialize the Q table to all 0s\n",
        "    rewards = []\n",
        "    mean_reward_for_1k_episode = 0\n",
        "\n",
        "    for e in range(episodes): #Run 1k training runs\n",
        "\n",
        "        state, _ = env.reset() #Part of OpenAI where you need to reset at the start of each run\n",
        "        total_reward = 0 #Set initial reward to 0\n",
        "        iteration = 0\n",
        "\n",
        "        if e % 1000 == 0:\n",
        "            mean_reward_for_1k_episode = float(mean_reward_for_1k_episode / 1000)\n",
        "            rewards.append(mean_reward_for_1k_episode)\n",
        "            print(f\"Episode: {e}, Mean reward: {mean_reward_for_1k_episode}, Epsilon: {epsilon}\")\n",
        "            mean_reward_for_1k_episode = 0\n",
        "\n",
        "\n",
        "        while True: #Loop until done == True\n",
        "            #IF random number is less than epsilon grab the random action else grab the argument max of Q[state]\n",
        "\n",
        "            current_state_index = env.current_pos[0] + env.current_pos[1]*env.observation_space[0] # Obtain the index of the state\n",
        "\n",
        "            action = compute_action(current_state_index, Q, epsilon, env) # Compute the action for the current state in function of the epsilon_greedy\n",
        "\n",
        "            posp1, _, reward, done = env.step(action) #Send your action to OpenAI and get back the tuple\n",
        "\n",
        "            state_tp1_index = posp1[0] + posp1[1]*env.observation_space[0]\n",
        "\n",
        "            total_reward += reward #Increment your reward\n",
        "            mean_reward_for_1k_episode += reward\n",
        "\n",
        "            Q[current_state_index][action] = Q[current_state_index][action] + alpha * (reward + gamma * np.max(Q[state_tp1_index]) - Q[current_state_index][action])\n",
        "\n",
        "             #Make sure to keep random at 10%\n",
        "\n",
        "            if done:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            if iteration >= max_iter_episode:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "\n",
        "        if epsilon>0.1:\n",
        "            epsilon *= np.exp(-epsilon_decay)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    delta_time = time.time() - start_time\n",
        "    print(f\"Time: {delta_time}\")\n",
        "\n",
        "    return Q, rewards, delta_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqBNpVXhMDGh"
      },
      "source": [
        "Effective running of the Q-Learning and saving of the trained Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ybGupCf-PAW",
        "outputId": "f38f1f25-a035-4903-93d0-fc2c13bbd379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: 0.04600000000003276, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: 0.8229999999999797, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: 0.9215999999999895, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: 0.9551999999999933, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: 0.9702999999999958, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: 0.979499999999997, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: 0.9872999999999982, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.9910999999999985, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.9934999999999989, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: 0.9943999999999992, Epsilon: 0.09997847803039572\n",
            "Time: 17.49876308441162\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: -1.3165999999993647, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: 0.4241999999999832, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: 0.714799999999971, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: 0.8233999999999768, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: 0.8780999999999834, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: 0.9167999999999884, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: 0.940299999999991, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.955099999999993, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.968399999999995, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: 0.9704999999999956, Epsilon: 0.09997847803039572\n",
            "Time: 17.91339659690857\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: -4.4174999999987925, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: -0.4076000000001701, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: 0.309200000000005, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: 0.5879999999999671, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: 0.7326999999999675, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: 0.8021999999999739, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: 0.8631999999999813, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.8995999999999852, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.921199999999988, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: 0.9400999999999913, Epsilon: 0.09997847803039572\n",
            "Time: 30.50448775291443\n"
          ]
        }
      ],
      "source": [
        "q_simple, rewards_q_simple, time_q_simple = q_learning(environment_simple)\n",
        "save('Trajectory - Simple - Q-Learning.pkl', q_simple)\n",
        "save('Rewards - Simple - Q-Learning.pkl', rewards_q_simple)\n",
        "save('Time - Simple - Q-Learning.pkl', time_q_simple)\n",
        "\n",
        "q_mid, rewards_q_mid, time_q_mid = q_learning(environment_mid)\n",
        "save('Trajectory - Mid - Q-Learning.pkl', q_mid)\n",
        "save('Rewards - Mid - Q-Learning.pkl', rewards_q_mid)\n",
        "save('Time - Mid - Q-Learning.pkl', time_q_mid)\n",
        "\n",
        "q_hard, rewards_q_hard, time_q_hard = q_learning(environment_hard)\n",
        "save('Trajectory - Hard - Q-Learning.pkl', q_hard)\n",
        "save('Rewards - Hard - Q-Learning.pkl', rewards_q_hard)\n",
        "save('Time - Hard - Q-Learning.pkl', time_q_hard)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tj708eRQB6r-",
        "outputId": "d6a9760b-87d1-410e-b033-a6af8af87a8c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6b2dcfda-4e60-4a2f-a2e2-18c19b648cd9\", \"Simple - Q-Learning.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5edd8bc7-712d-4eab-826c-1af23856ba97\", \"Mid - Q-Learning.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5ca11731-8c9b-4215-8d4b-0f1a4a0c993e\", \"Hard - Q-Learning.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5cd10a04-c89d-4d87-a893-fe6c87bff0b1\", \"Rewards - Simple - Q-Learning.pkl\", 90153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_457bbbc8-1bc7-4403-95de-07c1d0ffc5da\", \"Rewards - Mid - Q-Learning.pkl\", 90153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_1d7f3d11-2ca4-41bd-abb4-61ccfcf65050\", \"Rewards - Hard - Q-Learning.pkl\", 90153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_860e5c43-9532-4c83-b710-10cba12a34ca\", \"Time - Simple - Q-Learning.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f4f5eadf-f3b2-42c5-aec1-09d2521a1f08\", \"Time - Mid - Q-Learning.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b80f441e-9200-432f-802b-220e0ce7698d\", \"Time - Hard - Q-Learning.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Trajectory - Simple - Q-Learning.pkl')\n",
        "files.download('Trajectory - Mid - Q-Learning.pkl')\n",
        "files.download('Trajectory - Hard - Q-Learning.pkl')\n",
        "\n",
        "files.download('Rewards - Simple - Q-Learning.pkl')\n",
        "files.download('Rewards - Mid - Q-Learning.pkl')\n",
        "files.download('Rewards - Hard - Q-Learning.pkl')\n",
        "\n",
        "files.download('Time - Simple - Q-Learning.pkl')\n",
        "files.download('Time - Mid - Q-Learning.pkl')\n",
        "files.download('Time - Hard - Q-Learning.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xF7_Sc3K9AF"
      },
      "source": [
        "Loading Q-Learning trained Q-Table and checking if successful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WSOe6ObK73i",
        "outputId": "01f2ecd4-e759-4545-8fec-2d5e7bec714c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rewards simple 15017\n",
            "time simple 21.7232449054718\n",
            "rewards mid 15017\n",
            "time mid 22.858484029769897\n",
            "rewards hard 15017\n",
            "time hard 35.07642364501953\n"
          ]
        }
      ],
      "source": [
        "trained_q_simple = load('Trajectorty - Simple - Q-Learning.pkl')\n",
        "rewards_q_simple = load('Rewards - Simple - Q-Learning.pkl')\n",
        "time_q_simple = load('Time - Simple - Q-Learning.pkl')\n",
        "#print('simple', trained_q_simple)\n",
        "print('rewards simple', len(rewards_q_simple))\n",
        "print('time simple', time_q_simple)\n",
        "\n",
        "#trained_q_mid = load('Mid - Q-Learning.pkl')\n",
        "#print('mid', trained_q_mid)\n",
        "rewards_q_mid = load('Rewards - Mid - Q-Learning.pkl')\n",
        "time_q_mid = load('Time - Mid - Q-Learning.pkl')\n",
        "\n",
        "print('rewards mid', len(rewards_q_mid))\n",
        "print('time mid', time_q_mid)\n",
        "\n",
        "#trained_q_hard = load('Hard - Q-Learning.pkl')\n",
        "#print('hard', trained_q_hard)\n",
        "rewards_q_hard = load('Rewards - Hard - Q-Learning.pkl')\n",
        "time_q_hard = load('Time - Hard - Q-Learning.pkl')\n",
        "\n",
        "print('rewards hard', len(rewards_q_hard))\n",
        "print('time hard', time_q_hard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awOMcLSaLO6u"
      },
      "source": [
        "---\n",
        "\n",
        "# SARSA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbhauTm8Qx5q"
      },
      "outputs": [],
      "source": [
        "def sarsa(env, alpha=0.9, gamma=0.9,  epsilon=1, epsilon_decay=0.00025, episodes = 10001, max_iter_episode = 500):\n",
        "    start_time = time.time()\n",
        "    Q = np.zeros((env.grid_size[0]*env.grid_size[1], len(env.action_space)), dtype=np.float32) #Initialize the Q table to all 0s\n",
        "    rewards = []\n",
        "    mean_reward_for_1k_episode = 0\n",
        "\n",
        "    for e in range(episodes): #Run 1k training runs\n",
        "\n",
        "        state, _ = env.reset() #Part of OpenAI where you need to reset at the start of each run\n",
        "        total_reward = 0 #Set initial reward to 0\n",
        "        iteration = 0\n",
        "\n",
        "        if e % 1000 == 0:\n",
        "            mean_reward_for_1k_episode = float(mean_reward_for_1k_episode / 1000)\n",
        "            rewards.append(mean_reward_for_1k_episode)\n",
        "            print(f\"Episode: {e}, Mean reward: {mean_reward_for_1k_episode}, Epsilon: {epsilon}\")\n",
        "            mean_reward_for_1k_episode = 0\n",
        "\n",
        "        while True: #Loop until done == True\n",
        "            #IF random number is less than epsilon grab the random action else grab the argument max of Q[state]\n",
        "\n",
        "            current_state_index = env.current_pos[0] + env.current_pos[1]*env.observation_space[0] # Obtain the index of the state\n",
        "\n",
        "            action = compute_action(current_state_index, Q, epsilon, env) # Compute the action for the current state using Q-Table\n",
        "\n",
        "            posp1, _, reward, done = env.step(action) # Send the action to the environment and obtain the new position, the reward and the termination flag\n",
        "\n",
        "            state_tp1_index = posp1[0] + posp1[1]*env.observation_space[0] # Compute the index of the state at t+1\n",
        "            action_tp1 = compute_action(state_tp1_index, Q, epsilon, env) # Compute the action for the next state using Q-Table\n",
        "\n",
        "            total_reward += reward # Increment the reward\n",
        "            mean_reward_for_1k_episode += reward\n",
        "\n",
        "            Q[current_state_index][action] = Q[current_state_index][action] + alpha * (reward + gamma*Q[state_tp1_index][action_tp1] - Q[current_state_index][action])\n",
        "\n",
        "             #Make sure to keep random at 10%\n",
        "\n",
        "            if done:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            if iteration >= max_iter_episode:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "        if epsilon > 0.1:\n",
        "            epsilon *= np.exp(-epsilon_decay)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    delta_time = time.time() - start_time\n",
        "    print(f\"Time: {delta_time}\")\n",
        "    return Q, rewards, delta_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GIdo4_LQ7es"
      },
      "source": [
        "Effective running of SARSA and saving of the trained Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnzk9PyvRYN3",
        "outputId": "41d4411d-369b-4b4f-df78-34508feebc92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, Mean reward: 0.0, Epsilon: 1\n",
            "Episode: 1000, Mean reward: -1.0357000000001755, Epsilon: 0.7788007830714335\n",
            "Episode: 2000, Mean reward: 0.014000000000024236, Epsilon: 0.6065306597126773\n",
            "Episode: 3000, Mean reward: 0.3898999999999906, Epsilon: 0.4723665527410655\n",
            "Episode: 4000, Mean reward: 0.6622999999999706, Epsilon: 0.36787944117149507\n",
            "Episode: 5000, Mean reward: 0.7655999999999777, Epsilon: 0.2865047968602415\n",
            "Episode: 6000, Mean reward: 0.8072999999999805, Epsilon: 0.22313016014847828\n",
            "Episode: 7000, Mean reward: 0.8397999999999844, Epsilon: 0.17377394345048924\n",
            "Episode: 8000, Mean reward: 0.9152999999999925, Epsilon: 0.13533528323665214\n",
            "Episode: 9000, Mean reward: 0.8784999999999931, Epsilon: 0.10539922456189901\n",
            "Episode: 10000, Mean reward: 0.8834999999999935, Epsilon: 0.0999835106590795\n",
            "Time: 72.65182423591614\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 1\n",
            "Episode: 1000, Mean reward: -3.4005999999975582, Epsilon: 0.7788007830714335\n",
            "Episode: 2000, Mean reward: -1.6532999999993925, Epsilon: 0.6065306597126773\n",
            "Episode: 3000, Mean reward: -0.8300000000002479, Epsilon: 0.4723665527410655\n",
            "Episode: 4000, Mean reward: -0.4178000000001201, Epsilon: 0.36787944117149507\n",
            "Episode: 5000, Mean reward: 0.006200000000002651, Epsilon: 0.2865047968602415\n",
            "Episode: 6000, Mean reward: 0.3059999999999964, Epsilon: 0.22313016014847828\n",
            "Episode: 7000, Mean reward: 0.5312999999999757, Epsilon: 0.17377394345048924\n",
            "Episode: 8000, Mean reward: 0.49239999999997774, Epsilon: 0.13533528323665214\n",
            "Episode: 9000, Mean reward: 0.5531999999999856, Epsilon: 0.10539922456189901\n",
            "Episode: 10000, Mean reward: 0.7030999999999843, Epsilon: 0.0999835106590795\n",
            "Time: 107.41615772247314\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 1\n",
            "Episode: 1000, Mean reward: -6.669500000006829, Epsilon: 0.7788007830714335\n",
            "Episode: 2000, Mean reward: -4.175399999997626, Epsilon: 0.6065306597126773\n",
            "Episode: 3000, Mean reward: -2.6448999999986524, Epsilon: 0.4723665527410655\n",
            "Episode: 4000, Mean reward: -1.769599999999469, Epsilon: 0.36787944117149507\n",
            "Episode: 5000, Mean reward: -1.1939000000000168, Epsilon: 0.2865047968602415\n",
            "Episode: 6000, Mean reward: -0.8340000000001324, Epsilon: 0.22313016014847828\n",
            "Episode: 7000, Mean reward: -0.559800000000062, Epsilon: 0.17377394345048924\n",
            "Episode: 8000, Mean reward: -0.3019000000000012, Epsilon: 0.13533528323665214\n",
            "Episode: 9000, Mean reward: -0.1496999999999942, Epsilon: 0.10539922456189901\n",
            "Episode: 10000, Mean reward: -0.13929999999999665, Epsilon: 0.0999835106590795\n",
            "Time: 197.2619607448578\n"
          ]
        }
      ],
      "source": [
        "s_simple, rewards_s_simple, time_s_simple = sarsa(environment_simple)\n",
        "save('Trajectory - Simple - SARSA.pkl', s_simple)\n",
        "save('Rewards - Simple - SARSA.pkl', rewards_s_simple)\n",
        "save('Time - Simple - SARSA.pkl', time_s_simple)\n",
        "\n",
        "s_mid, rewards_s_mid, time_s_mid = sarsa(environment_mid)\n",
        "save('Trajectory - Mid - SARSA.pkl', s_mid)\n",
        "save('Rewards - Mid - SARSA.pkl', rewards_s_mid)\n",
        "save('Time - Mid - SARSA.pkl', time_s_mid)\n",
        "\n",
        "s_hard, rewards_s_hard, time_s_hard = sarsa(environment_hard)\n",
        "save('Trajectory - Hard - SARSA.pkl', s_hard)\n",
        "save('Rewards - Hard - SARSA.pkl', rewards_s_hard)\n",
        "save('Time - Hard - SARSA.pkl', time_s_hard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3rgdWRRgJW"
      },
      "source": [
        "Loading SARSA trained Q-Table and checking if successful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8usIp5HRq7K",
        "outputId": "865f0cbb-2db7-490c-9b92-28627f49604c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simple [[0.44567177 0.44728616 0.44286552 0.81373316]\n",
            " [0.4501721  0.6978284  0.4445712  0.70594054]\n",
            " [0.44446135 0.68351907 0.44634077 0.69530284]\n",
            " [0.6652588  0.67394763 0.44448787 0.5021277 ]\n",
            " [0.504806   0.68046397 0.5083074  0.5097628 ]\n",
            " [0.5007196  0.4999843  0.8650847  0.3661518 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.6634075  0.6457142  0.47332165 0.8003355 ]\n",
            " [0.66966087 0.80144984 0.67579293 0.66756135]\n",
            " [0.64987856 0.9199013  0.6313867  0.64306194]\n",
            " [0.44743183 0.4493168  0.44489297 0.4392594 ]\n",
            " [0.70790976 0.39580634 0.44302082 0.43136746]\n",
            " [0.68225974 0.40249524 0.44756815 0.5032215 ]\n",
            " [0.67955256 0.40019432 0.6793649  0.6803811 ]\n",
            " [0.68197215 0.6691524  0.6737774  0.67031896]\n",
            " [0.506075   0.6755831  0.67561656 0.6654265 ]\n",
            " [0.65725535 0.67022943 0.528631   0.8997505 ]\n",
            " [0.47207808 0.85975456 0.6521792  0.65015733]\n",
            " [0.682654   0.65441257 0.6669527  0.6702138 ]\n",
            " [0.67464066 0.8960379  0.64969885 0.68135995]\n",
            " [0.4472489  0.4395988  0.4448456  0.4194471 ]\n",
            " [0.39637545 0.62692505 0.4583643  0.42319477]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.6751587  0.46334675 0.5768832  0.6634563 ]\n",
            " [0.5035569  0.4606547  0.6691587  0.6574348 ]\n",
            " [0.6640522  0.4884401  0.66330963 0.66284597]\n",
            " [0.6669873  0.92077506 0.79475355 0.8679143 ]\n",
            " [0.6814921  0.7142257  0.8622353  0.6525082 ]\n",
            " [0.6440053  0.8956823  0.66331226 0.65670484]\n",
            " [0.44355717 0.360578   0.36941013 0.37273198]\n",
            " [0.22617729 0.6028319  0.43357733 0.4104158 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.7184118  0.715154   0.6633051  0.7062432 ]\n",
            " [0.4617705  0.700397   0.7132428  0.8529414 ]\n",
            " [0.48667887 0.48930064 0.5042189  0.8839329 ]\n",
            " [0.6721523  0.8694387  0.7918634  0.9140254 ]\n",
            " [0.6536045  0.94039416 0.8687225  0.68584746]\n",
            " [0.6901672  0.90363485 0.86057794 0.8847945 ]\n",
            " [0.4405398  0.4413835  0.44169995 0.440124  ]\n",
            " [0.46501434 0.6007354  0.4429562  0.46693128]\n",
            " [0.40543732 0.8046945  0.4589269  0.4402955 ]\n",
            " [0.5124023  0.49271366 0.7248742  0.46262535]\n",
            " [0.71259516 0.70822936 0.60677487 0.48692313]\n",
            " [0.7061226  0.6700948  0.48472977 0.48913988]\n",
            " [0.64228505 0.4900257  0.5930425  0.88894415]\n",
            " [0.87760496 0.87094164 0.87037414 0.88677526]\n",
            " [0.8949708  0.950968   0.8947271  0.89470345]\n",
            " [0.86921996 0.92094207 0.90355045 0.90371305]\n",
            " [0.59096    0.42117876 0.4328027  0.43713483]\n",
            " [0.4620544  0.71162933 0.45551774 0.47300056]\n",
            " [0.46732083 0.61202157 0.55907434 0.71141577]\n",
            " [0.7175596  0.7604368  0.5553728  0.7256488 ]\n",
            " [0.70688415 0.57512206 0.70738494 0.48933867]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.68350035 0.8080901  0.7505975  0.91352737]\n",
            " [0.9042417  0.9418478  0.940232   0.9410858 ]\n",
            " [0.911584   0.92246807 0.9502551  0.91397816]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.43615875 0.45490193 0.35252568 0.45723107]\n",
            " [0.47387853 0.4591697  0.5626364  0.83100367]\n",
            " [0.752993   0.6171306  0.61788815 0.6372003 ]\n",
            " [0.63176674 0.53506577 0.8307374  0.57237446]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.9393929  0.960596   0.8148022  0.8172406 ]\n",
            " [0.93170905 0.97010773 0.95097905 0.9228541 ]\n",
            " [0.9230308  0.92380303 0.9420479  0.9316918 ]\n",
            " [0.403424   0.49637583 0.40021342 0.45739672]\n",
            " [0.45468238 0.4611935  0.8426293  0.4593485 ]\n",
            " [0.62595296 0.6411583  0.46889845 0.47984362]\n",
            " [0.6189963  0.47244427 0.48697382 0.57299787]\n",
            " [0.623646   0.9398477  0.79602003 0.8651562 ]\n",
            " [0.8201318  0.94988644 0.573021   0.6976185 ]\n",
            " [0.67247355 0.9232853  0.88773036 0.92348665]\n",
            " [0.9232024  0.9323641  0.93187815 0.97010976]\n",
            " [0.9605829  0.98010004 0.94169176 0.94186157]\n",
            " [0.95127827 0.9153193  0.96992165 0.979904  ]\n",
            " [0.47572944 0.4743231  0.47523397 0.65528834]\n",
            " [0.45522073 0.6199303  0.47481516 0.46547666]\n",
            " [0.81481206 0.89041907 0.47007293 0.47790983]\n",
            " [0.80931854 0.86273664 0.8786578  0.9403806 ]\n",
            " [0.8782084  0.895951   0.93039197 0.95078003]\n",
            " [0.88753456 0.8851859  0.88133043 0.95940155]\n",
            " [0.9140762  0.9512726  0.9129203  0.9129766 ]\n",
            " [0.923225   0.98010004 0.9216158  0.97950745]\n",
            " [0.97010976 0.99       0.97029895 0.9153193 ]\n",
            " [0.92380303 1.         0.98009807 0.99      ]\n",
            " [0.47096795 0.4706116  0.47341233 0.63805467]\n",
            " [0.45244077 0.46954498 0.4705245  0.86895275]\n",
            " [0.51313597 0.869321   0.47473496 0.869792  ]\n",
            " [0.92941964 0.5964805  0.87179786 0.597875  ]\n",
            " [0.86979717 0.6821411  0.6064649  0.600509  ]\n",
            " [0.6987155  0.87895566 0.6812865  0.9225777 ]\n",
            " [0.8885658  0.9043696  0.88819665 0.9703001 ]\n",
            " [0.969925   0.96022516 0.9321699  0.99      ]\n",
            " [0.9800962  0.99       0.9509148  1.        ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "mid [[ 0.1424276   0.1431843   0.18298288  0.6003691 ]\n",
            " [ 0.5942043   0.20754096  0.14246525  0.5939285 ]\n",
            " [ 0.02428232  0.8056469   0.02883808  0.5975223 ]\n",
            " [ 0.12089164  0.8237561   0.12312717  0.02043976]\n",
            " [ 0.1243939   0.5561753   0.1197564   0.01957284]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.3257401   0.32294434  0.32740074  0.64273554]\n",
            " [ 0.4315584   0.69383675  0.51167524  0.37575454]\n",
            " [ 0.4352138   0.3749626   0.5134975   0.5237126 ]\n",
            " [ 0.21455836  0.14428902  0.14388026  0.14244786]\n",
            " [ 0.69177073  0.10180416  0.14208946  0.1201145 ]\n",
            " [ 0.03446975  0.11050113  0.10208285  0.5956718 ]\n",
            " [ 0.11972867  0.1260393   0.5927625   0.817537  ]\n",
            " [ 0.02488958  0.849364    0.11857031  0.01931834]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.32618192  0.44190064  0.44556487  0.51540196]\n",
            " [ 0.51826525  0.90083027  0.5569407   0.5170433 ]\n",
            " [ 0.59830284  0.91389567  0.3947066   0.37639537]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.11111978  0.7265389   0.0034893   0.10408388]\n",
            " [ 0.12757532  0.7265827   0.09726053  0.3994923 ]\n",
            " [ 0.408912    0.02463239  0.72272134  0.5645082 ]\n",
            " [ 0.21768706  0.32269192  0.13771237  0.7676143 ]\n",
            " [ 0.41125163  0.70253336  0.32028663  0.40742853]\n",
            " [ 0.44374925  0.5137206   0.8279445   0.586368  ]\n",
            " [ 0.7259382   0.5223062   0.72542197  0.5894631 ]\n",
            " [ 0.59229803  0.83111984  0.7855293   0.8783241 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.10997849  0.67586386  0.00939789  0.70600504]\n",
            " [ 0.49895343  0.0617434   0.7073993   0.42211178]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.5074498   0.52241296  0.5755792   0.6141895 ]\n",
            " [ 0.6194338   0.8240127   0.51595616  0.579376  ]\n",
            " [ 0.5816424   0.92281383  0.80622804  0.84218746]\n",
            " [ 0.8119066   0.9476137   0.79950476  0.8338338 ]\n",
            " [ 0.16942789  0.18413305  0.27277824  0.7181529 ]\n",
            " [ 0.20651297  0.20426169  0.40155444  0.6752924 ]\n",
            " [ 0.50319433  0.6822967   0.6507996   0.658734  ]\n",
            " [ 0.05388802  0.20372309  0.50800943 -0.04140861]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.6749109   0.54428804  0.52565795  0.5198694 ]\n",
            " [ 0.64621544  0.63933045  0.7733492   0.8268478 ]\n",
            " [ 0.8324994   0.9497972   0.8269725   0.8283893 ]\n",
            " [ 0.8353348   0.9605616   0.8271103   0.837208  ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.26106656  0.19732799  0.09889492  0.75185007]\n",
            " [-0.03382231  0.44825763  0.20105459  0.83770424]\n",
            " [ 0.70185333  0.707165    0.7433024   0.9151701 ]\n",
            " [ 0.70001966  0.711816    0.75830287  0.9319331 ]\n",
            " [ 0.57289416  0.62695986  0.45335448  0.92366123]\n",
            " [ 0.8281546   0.55058724  0.6436175   0.82223123]\n",
            " [ 0.6552246   0.7426048   0.64000046  0.9592809 ]\n",
            " [ 0.8283198   0.970299    0.8212965   0.95898783]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.35303232  0.19312921  0.4525183   0.76241136]\n",
            " [ 0.85625523  0.42333272  0.44857973  0.38242385]\n",
            " [ 0.71075076  0.3827519   0.83808714  0.71258855]\n",
            " [ 0.7731736   0.7180218   0.7008446   0.625611  ]\n",
            " [ 0.63430274  0.7182815   0.7699647   0.63085526]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.8374831   0.98010004  0.8605747   0.8531523 ]\n",
            " [ 0.07901178  0.5590758   0.16676395 -0.0725259 ]\n",
            " [-0.07429671  0.02337185  0.08320998  0.6548437 ]\n",
            " [ 0.77989256  0.19010964  0.4685094   0.45324627]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.78921187  0.8409293   0.7241116   0.795244  ]\n",
            " [ 0.7949015   0.96057385  0.8851968   0.72393304]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.97029805  0.99        0.8801      0.97029793]\n",
            " [ 0.55723417  0.55529463  0.5535597   0.4873905 ]\n",
            " [ 0.18042582  0.5788975  -0.04683734  0.6635701 ]\n",
            " [ 0.6026757  -0.0214261   0.18464035  0.36935875]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.9038978   0.81588155  0.8487977   0.96057385]\n",
            " [ 0.8197972   0.74435025  0.9465956   0.970299  ]\n",
            " [ 0.83176637  0.98010004  0.96059227  0.97902095]\n",
            " [ 0.8801      0.99        0.9692209   0.9702999 ]\n",
            " [ 0.9801      1.          0.98010004  0.99      ]\n",
            " [ 0.551755    0.55908006  0.5591326   0.5484724 ]\n",
            " [-0.03854797  0.5484979   0.55390024  0.8121775 ]\n",
            " [ 0.6772598   0.71184367 -0.0294175   0.7130007 ]\n",
            " [ 0.7341259   0.72481036  0.72014236  0.74168426]\n",
            " [ 0.69209695  0.6862458   0.7135785   0.7552878 ]\n",
            " [ 0.94687057  0.7371661   0.75513816  0.74435025]\n",
            " [ 0.9599309   0.9601243   0.7495636   0.98010004]\n",
            " [ 0.9702952   0.96078634  0.9702934   0.9703067 ]\n",
            " [ 0.9801      0.989999    0.97990686  1.        ]\n",
            " [ 0.          0.          0.          0.        ]]\n",
            "hard [[-0.01838733 -0.01267159 -0.01368699 -0.0193904 ]\n",
            " [-0.08163207 -0.08963685 -0.01793873 -0.068295  ]\n",
            " [-0.02293469 -0.06316753 -0.03451106 -0.06271549]\n",
            " [-0.10797756 -0.03310829 -0.06830779 -0.11895342]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.19165029 -0.26805323 -0.26799327 -0.383268  ]\n",
            " [-0.38752356 -0.40950683 -0.1916347  -0.6670537 ]\n",
            " [-0.7532703  -0.77512884 -0.3119838  -0.78315264]\n",
            " [-0.88924676 -0.80829334 -0.59357995 -0.75601625]\n",
            " [-0.01277604 -0.01627237 -0.03562086 -0.01358959]\n",
            " [-0.07458296 -0.11851329 -0.01276015 -0.09959544]\n",
            " [-0.03001312 -0.16631375 -0.10767863 -0.11270674]\n",
            " [-0.11684296 -0.09576972 -0.02196078 -0.13085954]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.18003866 -0.4352955  -0.40858302 -0.26186904]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.5665099  -0.68128663 -0.7542214  -0.66990155]\n",
            " [-0.04251118 -0.13932858 -0.13584866 -0.01233288]\n",
            " [-0.01007279 -0.13721226 -0.12312213 -0.12911966]\n",
            " [-0.07588761 -0.18258135 -0.14173882 -0.1350481 ]\n",
            " [-0.04085248 -0.14235985 -0.12426402 -0.14606646]\n",
            " [-0.1554751  -0.23623633 -0.10709427 -0.41972303]\n",
            " [-0.17754716 -0.14362882 -0.12151676 -0.459855  ]\n",
            " [-0.18060619 -0.1137665  -0.06592258 -0.17092289]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.57568216 -0.5645995  -0.72632444 -0.5308014 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.0373408  -0.18753786 -0.14767992 -0.16958502]\n",
            " [-0.06155614 -0.15680881 -0.18473905 -0.14107871]\n",
            " [-0.2329784  -0.24831592 -0.08084831 -0.17556581]\n",
            " [-0.1403379  -0.2812289  -0.20476851 -0.26827475]\n",
            " [-0.06682602 -0.29359868 -0.36124477 -0.51705104]\n",
            " [-0.5907472  -0.3640078  -0.37668937 -0.5417424 ]\n",
            " [-0.62929296 -0.57085377 -0.39056274 -0.61436564]\n",
            " [-0.6958564  -0.4593427  -0.6130374  -0.6303027 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.33998123 -0.5635427  -0.4970866  -0.5408644 ]\n",
            " [-0.38648885 -0.64370507 -0.6455399  -0.4753485 ]\n",
            " [-0.59811854 -0.58336645 -0.4714622  -0.5757587 ]\n",
            " [-0.9249125  -0.96306944 -1.0012532  -1.0993634 ]\n",
            " [-1.117732   -1.1440812  -0.93013453 -1.2191777 ]\n",
            " [-1.2201557  -1.2653303  -1.1907308  -1.1516272 ]\n",
            " [-1.2670816  -0.60381514 -1.161734   -1.5626174 ]\n",
            " [-1.6801662  -1.3983352  -1.0748496  -1.6923004 ]\n",
            " [-1.2510488  -1.1716808  -1.5870143  -1.3556466 ]\n",
            " [-0.7235278  -1.4681894  -0.74986243 -0.62446654]\n",
            " [-0.24313398 -0.70989823 -1.3696309  -0.6512715 ]\n",
            " [-0.5438348  -0.6623636  -1.271716   -0.5665411 ]\n",
            " [-0.58470464 -0.8543473  -0.561911   -0.5917745 ]\n",
            " [-0.99870545 -0.9000979  -1.041796   -1.0178258 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-1.2934679  -0.40709195 -1.3552151  -1.280146  ]\n",
            " [-1.5745934  -1.4852914  -1.0153427  -1.1496676 ]\n",
            " [-1.5453156  -1.4863265  -1.4364501  -1.1664237 ]\n",
            " [-1.3282877  -1.6444207  -1.5020407  -0.53083545]\n",
            " [-0.5923658  -0.718723   -0.6777216  -1.5658458 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-1.0326086  -0.880725   -1.056351   -1.1554664 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-1.162048   -0.42160943 -1.2349195  -1.6735494 ]\n",
            " [-1.4234067  -1.7882231  -1.177829   -1.1271453 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-1.04444    -0.89712346 -1.0064641  -1.0290519 ]\n",
            " [-0.9463652  -1.057297   -0.87536526 -0.9326911 ]\n",
            " [-1.0653975  -0.4476327  -0.8262168  -0.6206135 ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.        ]\n",
            " [-0.00353448 -0.2762897  -0.03957358  0.06457656]\n",
            " [-0.16385035  0.0822586   0.04902069  0.03263004]\n",
            " [-0.03291882 -0.02877124  0.03106229  0.9775181 ]\n",
            " [ 0.77047265  0.9745976   0.83926994  0.9899019 ]\n",
            " [ 0.88901097  1.          0.9268574   0.        ]\n",
            " [-0.8680957  -0.9213138  -0.9086234  -0.89971006]\n",
            " [-0.9294272  -0.91343427 -0.8508989  -0.31978184]\n",
            " [-0.45351294 -0.89835376 -0.6698833  -0.00956528]\n",
            " [-0.30453765 -0.28997317 -0.32205355 -0.686425  ]\n",
            " [-0.0880785   0.03984078 -0.29259223  0.06244925]\n",
            " [-0.03118046 -0.3303304  -0.3215702   0.03754476]\n",
            " [ 0.08158608 -0.0142335   0.00508927 -0.01232587]\n",
            " [-0.01436794 -0.02021754 -0.08918333  0.9796207 ]\n",
            " [ 0.951128   -0.04874479  0.9753454   1.        ]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "trained_s_simple = load('Trajectory - Simple - SARSA.pkl')\n",
        "print('simple', trained_s_simple)\n",
        "\n",
        "trained_s_mid = load('Trajectory - Mid - SARSA.pkl')\n",
        "print('mid', trained_s_mid)\n",
        "\n",
        "trained_s_hard = load('Trajectory - Hard - SARSA.pkl')\n",
        "print('hard',trained_s_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "N2jNnqvmEh89",
        "outputId": "e770b062-e95a-4407-e46d-4f7a729ff5a4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cfa805cf-6a06-4568-bc0e-4d78ff0e7cff\", \"Simple - SARSA.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4f0579a9-fe3f-4a75-8374-722483ea744a\", \"Mid - SARSA.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_73f663ad-f654-4c63-a26b-9df3c75b3e0a\", \"Hard - SARSA.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_3838897a-85d3-4c16-8839-0bb71b8deb7c\", \"Rewards - Simple - SARSA.pkl\", 89551)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2c4c7378-9561-43ae-bfdf-dda9613f1e69\", \"Rewards - Mid - SARSA.pkl\", 89705)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8b7cd1c8-d0f2-49f3-a783-339fb8230f8c\", \"Rewards - Hard - SARSA.pkl\", 80696)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4412da48-f02a-48db-bfcb-8c5c69b71507\", \"Time - Simple - SARSA.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_dc1c4fd1-d2ea-4a02-9939-a2193159aab3\", \"Time - Mid - SARSA.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e06e04a1-f16b-470a-8bbb-f1905ca440ec\", \"Time - Hard - SARSA.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Trajectory - Simple - SARSA.pkl')\n",
        "files.download('Trajectory - Mid - SARSA.pkl')\n",
        "files.download('Trajectory - Hard - SARSA.pkl')\n",
        "\n",
        "files.download('Rewards - Simple - SARSA.pkl')\n",
        "files.download('Rewards - Mid - SARSA.pkl')\n",
        "files.download('Rewards - Hard - SARSA.pkl')\n",
        "\n",
        "files.download('Time - Simple - SARSA.pkl')\n",
        "files.download('Time - Mid - SARSA.pkl')\n",
        "files.download('Time - Hard - SARSA.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wHNHgF7R0Aq"
      },
      "source": [
        "---\n",
        "\n",
        "# Alternating Q-Learning / SARSA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv7jMbGIR219"
      },
      "outputs": [],
      "source": [
        "def alternating(env, alpha=1, gamma=0.9,  epsilon=0.99, epsilon_decay=0.00025, episodes = 10001, max_iter_episode = 500):\n",
        "    start_time = time.time()\n",
        "    Q = np.zeros((env.grid_size[0]*env.grid_size[1], len(env.action_space)), dtype=np.float32) #Initialize the Q table to all 0s\n",
        "    rewards = []\n",
        "    mean_reward_for_1k_episode = 0\n",
        "\n",
        "    for e in range(episodes): #Run 1k training runs\n",
        "\n",
        "        state, _ = env.reset() #Part of OpenAI where you need to reset at the start of each run\n",
        "        total_reward = 0 #Set initial reward to 0\n",
        "        iteration = 0\n",
        "\n",
        "        if e % 1000 == 0:\n",
        "            mean_reward_for_1k_episode = float(mean_reward_for_1k_episode / 1000)\n",
        "            rewards.append(mean_reward_for_1k_episode)\n",
        "            print(f\"Episode: {e}, Mean reward: {mean_reward_for_1k_episode}, Epsilon: {epsilon}\")\n",
        "            mean_reward_for_1k_episode = 0\n",
        "\n",
        "        while True: #Loop until done == True\n",
        "\n",
        "            random_num = random.random() # Generate a random number between 0 and 1\n",
        "\n",
        "            current_state_index = env.current_pos[0] + env.current_pos[1]*env.observation_space[0] # Obtain the index of the state\n",
        "\n",
        "            action = compute_action(current_state_index, Q, epsilon, env) # Compute the action for the current state using Q-Table\n",
        "\n",
        "            posp1, _, reward, done = env.step(action) # Send the action to the environment and obtain the new position, the reward and the termination flag\n",
        "\n",
        "            state_tp1_index = posp1[0] + posp1[1]*env.observation_space[0] # Compute the index of the state at t+1\n",
        "            action_tp1 = compute_action(state_tp1_index, Q, epsilon, env) # Compute the action for the next state using Q-Table\n",
        "\n",
        "            total_reward += reward # Increment the reward\n",
        "            mean_reward_for_1k_episode += reward\n",
        "\n",
        "            if (random_num <= 0.5): # We use Q-learning\n",
        "                Q[current_state_index][action] = Q[current_state_index][action] + alpha * (reward + gamma * np.max(Q[state_tp1_index]) - Q[current_state_index][action])\n",
        "\n",
        "            else: # We use SARSA\n",
        "                Q[current_state_index][action] = Q[current_state_index][action] + alpha * (reward + gamma*Q[state_tp1_index][action_tp1] - Q[current_state_index][action])\n",
        "\n",
        "            if done:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            if iteration >= max_iter_episode:\n",
        "                #print(f\"Episode: {e}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "        if epsilon > 0.1:\n",
        "            epsilon *= np.exp(-epsilon_decay)\n",
        "\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    delta_time = time.time() - start_time\n",
        "    print(f\"Time: {delta_time}\")\n",
        "\n",
        "    return Q, rewards, delta_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6YOUC1AccKB"
      },
      "source": [
        "Effective running of alternating Q-Learning/SARSA and saving of the trained Q-Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb-whO11ccrf",
        "outputId": "99175676-bcd3-49ee-ebb0-b23d2752ba90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: -0.3400000000001813, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: 0.6193999999999744, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: 0.793299999999979, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: 0.8805999999999828, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: 0.9251999999999906, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: 0.9428999999999921, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: 0.9587999999999939, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.9723999999999964, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.9846999999999978, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: 0.9808999999999964, Epsilon: 0.09997847803039572\n",
            "Time: 36.44415831565857\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: -2.2387999999984456, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: -0.04959999999995778, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: 0.4400999999999785, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: 0.6513999999999676, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: 0.744899999999973, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: 0.8258999999999767, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: 0.8674999999999828, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.917299999999988, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.9218999999999895, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: 0.9350999999999914, Epsilon: 0.09997847803039572\n",
            "Time: 41.443246603012085\n",
            "Episode: 0, Mean reward: 0.0, Epsilon: 0.99\n",
            "Episode: 1000, Mean reward: -6.468900000006797, Epsilon: 0.7710127752407178\n",
            "Episode: 2000, Mean reward: -3.3507999999975584, Epsilon: 0.60046535311555\n",
            "Episode: 3000, Mean reward: -1.9339999999989932, Epsilon: 0.467642887213655\n",
            "Episode: 4000, Mean reward: -1.089000000000103, Epsilon: 0.36420064675978037\n",
            "Episode: 5000, Mean reward: -0.4485000000001307, Epsilon: 0.28363974889163973\n",
            "Episode: 6000, Mean reward: -0.36540000000001055, Epsilon: 0.22089885854699348\n",
            "Episode: 7000, Mean reward: -0.059699999999973025, Epsilon: 0.1720362040159844\n",
            "Episode: 8000, Mean reward: 0.044900000000008, Epsilon: 0.1339819304042855\n",
            "Episode: 9000, Mean reward: 0.23060000000001007, Epsilon: 0.10434523231627953\n",
            "Episode: 10000, Mean reward: -0.04900000000000072, Epsilon: 0.09997847803039572\n",
            "Time: 145.92025876045227\n"
          ]
        }
      ],
      "source": [
        "a_simple, rewards_a_simple, time_a_simple = alternating(environment_simple)\n",
        "save('Trajectory - Simple - Alternating.pkl', a_simple)\n",
        "save('Rewards - Simple - Alternating.pkl', rewards_a_simple)\n",
        "save('Time - Simple - Alternating.pkl', time_a_simple)\n",
        "\n",
        "a_mid, rewards_a_mid, time_a_mid = alternating(environment_mid)\n",
        "save('Trajectory - Mid - Alternating.pkl', a_mid)\n",
        "save('Rewards - Mid - Alternating.pkl', rewards_a_mid)\n",
        "save('Time - Mid - Alternating.pkl', time_a_mid)\n",
        "\n",
        "a_hard, rewards_a_hard, time_a_hard = alternating(environment_hard)\n",
        "save('Trajectort - Hard - Alternating.pkl', a_hard)\n",
        "save('Rewards - Hard - Alternating.pkl', rewards_a_hard)\n",
        "save('Time - Hard - Alternating.pkl', time_a_hard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpPjMa9NceFi"
      },
      "source": [
        "Loading alternating trained Q-Table and checking if successful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztzTcbx-cdPk",
        "outputId": "c92dd2cf-1157-49d1-85b8-134e9bd2eee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.06461082 0.0717898  0.06461082 0.0717898 ]\n",
            " [0.0717898  0.06461082 0.07976644 0.07976644]\n",
            " [0.0717898  0.06461082 0.05814974 0.07976644]\n",
            " [0.09847709 0.05233477 0.08862938 0.10941899]\n",
            " [0.09847709 0.12157665 0.09847709 0.10941899]\n",
            " [0.12157665 0.18530202 0.09847709 0.13508517]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.16677181 0.22876792 0.25418657 0.16677181]\n",
            " [0.13508517 0.13508517 0.25418657 0.0717898 ]\n",
            " [0.12157665 0.28242952 0.07976644 0.0717898 ]\n",
            " [0.06461082 0.15009463 0.0717898  0.07976644]\n",
            " [0.0717898  0.05814974 0.04710129 0.0717898 ]\n",
            " [0.0717898  0.0717898  0.04239116 0.05233477]\n",
            " [0.09847709 0.09847709 0.09847709 0.05814974]\n",
            " [0.12157665 0.12157665 0.12157665 0.18530202]\n",
            " [0.13508517 0.13508517 0.16677181 0.20589113]\n",
            " [0.20589113 0.20589113 0.12157665 0.25418657]\n",
            " [0.25418657 0.3138106  0.22876792 0.25418657]\n",
            " [0.13508517 0.28242952 0.18530202 0.22876792]\n",
            " [0.20589113 0.34867844 0.25418657 0.25418657]\n",
            " [0.0717898  0.15009463 0.05814974 0.05814974]\n",
            " [0.04239116 0.06461082 0.06461082 0.05814974]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.13508517 0.16677181 0.15009463 0.13508517]\n",
            " [0.16677181 0.15009463 0.22876792 0.22876792]\n",
            " [0.22876792 0.3138106  0.22876792 0.20589113]\n",
            " [0.28242952 0.34867844 0.22876792 0.25418657]\n",
            " [0.25418657 0.47829688 0.28242952 0.25418657]\n",
            " [0.3138106  0.4304672  0.25418657 0.25418657]\n",
            " [0.05814974 0.08862938 0.16677181 0.16677181]\n",
            " [0.06461082 0.18530202 0.08862938 0.16677181]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.13508517 0.13508517 0.15009463 0.18530202]\n",
            " [0.12157665 0.18530202 0.20589113 0.28242952]\n",
            " [0.15009463 0.22876792 0.22876792 0.4304672 ]\n",
            " [0.3138106  0.25418657 0.28242952 0.38742048]\n",
            " [0.28242952 0.4304672  0.22876792 0.38742048]\n",
            " [0.25418657 0.22876792 0.38742048 0.38742048]\n",
            " [0.05233477 0.20589113 0.08862938 0.09847709]\n",
            " [0.16677181 0.15009463 0.10941899 0.16677181]\n",
            " [0.15009463 0.20589113 0.16677181 0.12157665]\n",
            " [0.20589113 0.25418657 0.16677181 0.16677181]\n",
            " [0.13508517 0.15009463 0.22876792 0.16677181]\n",
            " [0.22876792 0.18530202 0.18530202 0.18530202]\n",
            " [0.25418657 0.22876792 0.16677181 0.3138106 ]\n",
            " [0.28242952 0.4304672  0.28242952 0.4304672 ]\n",
            " [0.38742048 0.4304672  0.38742048 0.25418657]\n",
            " [0.20589113 0.531441   0.28242952 0.34867844]\n",
            " [0.09847709 0.0717898  0.22876792 0.15009463]\n",
            " [0.12157665 0.15009463 0.08862938 0.15009463]\n",
            " [0.16677181 0.22876792 0.15009463 0.18530202]\n",
            " [0.22876792 0.3138106  0.25418657 0.20589113]\n",
            " [0.18530202 0.16677181 0.22876792 0.16677181]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.25418657 0.38742048 0.34867844 0.59049   ]\n",
            " [0.4304672  0.531441   0.34867844 0.34867844]\n",
            " [0.47829688 0.47829688 0.47829688 0.34867844]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.15009463 0.22876792 0.16677181 0.18530202]\n",
            " [0.16677181 0.22876792 0.18530202 0.18530202]\n",
            " [0.13508517 0.25418657 0.20589113 0.22876792]\n",
            " [0.16677181 0.38742048 0.25418657 0.25418657]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.4304672  0.4304672  0.4304672  0.34867844]\n",
            " [0.4304672  0.47829688 0.4304672  0.47829688]\n",
            " [0.47829688 0.531441   0.34867844 0.531441  ]\n",
            " [0.16677181 0.13508517 0.15009463 0.20589113]\n",
            " [0.16677181 0.22876792 0.18530202 0.20589113]\n",
            " [0.20589113 0.22876792 0.16677181 0.28242952]\n",
            " [0.22876792 0.25418657 0.18530202 0.28242952]\n",
            " [0.28242952 0.25418657 0.25418657 0.3138106 ]\n",
            " [0.28242952 0.3138106  0.3138106  0.47829688]\n",
            " [0.38742048 0.3138106  0.34867844 0.6561    ]\n",
            " [0.47829688 0.729      0.47829688 0.729     ]\n",
            " [0.531441   0.531441   0.531441   0.6561    ]\n",
            " [0.47829688 0.9        0.59049    0.531441  ]\n",
            " [0.15009463 0.16677181 0.13508517 0.12157665]\n",
            " [0.22876792 0.18530202 0.15009463 0.15009463]\n",
            " [0.25418657 0.20589113 0.16677181 0.15009463]\n",
            " [0.16677181 0.22876792 0.18530202 0.28242952]\n",
            " [0.28242952 0.3138106  0.28242952 0.3138106 ]\n",
            " [0.4304672  0.34867844 0.34867844 0.25418657]\n",
            " [0.34867844 0.28242952 0.3138106  0.729     ]\n",
            " [0.4304672  0.81       0.4304672  0.6561    ]\n",
            " [0.59049    0.9        0.59049    0.729     ]\n",
            " [0.81       1.         0.81       0.9       ]\n",
            " [0.16677181 0.16677181 0.13508517 0.22876792]\n",
            " [0.22876792 0.20589113 0.16677181 0.18530202]\n",
            " [0.22876792 0.22876792 0.20589113 0.20589113]\n",
            " [0.25418657 0.22876792 0.25418657 0.25418657]\n",
            " [0.28242952 0.3138106  0.28242952 0.34867844]\n",
            " [0.38742048 0.34867844 0.3138106  0.3138106 ]\n",
            " [0.4304672  0.3138106  0.34867844 0.81      ]\n",
            " [0.47829688 0.4304672  0.729      0.9       ]\n",
            " [0.6561     0.9        0.81       1.        ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "[[0.07976644 0.07976644 0.07976644 0.0717898 ]\n",
            " [0.07976644 0.07976644 0.07976644 0.10941899]\n",
            " [0.09847709 0.08862938 0.09847709 0.09847709]\n",
            " [0.10941899 0.10941899 0.10941899 0.13508517]\n",
            " [0.12157665 0.15009463 0.12157665 0.13508517]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.13508517 0.20589113 0.13508517 0.15009463]\n",
            " [0.18530202 0.22876792 0.15009463 0.13508517]\n",
            " [0.15009463 0.25418657 0.13508517 0.15009463]\n",
            " [0.07976644 0.0717898  0.07976644 0.09847709]\n",
            " [0.0717898  0.07976644 0.07976644 0.09847709]\n",
            " [0.10941899 0.09847709 0.06461082 0.09847709]\n",
            " [0.09847709 0.09847709 0.10941899 0.06461082]\n",
            " [0.13508517 0.28242952 0.16677181 0.15009463]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.20589113 0.22876792 0.22876792 0.22876792]\n",
            " [0.20589113 0.25418657 0.20589113 0.22876792]\n",
            " [0.25418657 0.4304672  0.25418657 0.25418657]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.09847709 0.10941899 0.10941899 0.10941899]\n",
            " [0.08862938 0.20589113 0.09847709 0.08862938]\n",
            " [0.06461082 0.12157665 0.09847709 0.28242952]\n",
            " [0.18530202 0.18530202 0.13508517 0.18530202]\n",
            " [0.16677181 0.28242952 0.18530202 0.18530202]\n",
            " [0.20589113 0.20589113 0.20589113 0.25418657]\n",
            " [0.13508517 0.25418657 0.22876792 0.47829688]\n",
            " [0.28242952 0.38742048 0.25418657 0.22876792]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.10941899 0.25418657 0.12157665 0.12157665]\n",
            " [0.10941899 0.22876792 0.10941899 0.12157665]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.28242952 0.20589113 0.3138106  0.18530202]\n",
            " [0.22876792 0.3138106  0.28242952 0.34867844]\n",
            " [0.22876792 0.34867844 0.25418657 0.20589113]\n",
            " [0.34867844 0.3138106  0.34867844 0.3138106 ]\n",
            " [0.13508517 0.07976644 0.07976644 0.18530202]\n",
            " [0.10941899 0.10941899 0.0717898  0.12157665]\n",
            " [0.10941899 0.10941899 0.12157665 0.3138106 ]\n",
            " [0.25418657 0.34867844 0.25418657 0.25418657]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.20589113 0.20589113 0.22876792 0.20589113]\n",
            " [0.25418657 0.22876792 0.22876792 0.34867844]\n",
            " [0.22876792 0.3138106  0.3138106  0.38742048]\n",
            " [0.3138106  0.38742048 0.34867844 0.34867844]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.12157665 0.15009463 0.25418657 0.34867844]\n",
            " [0.25418657 0.20589113 0.25418657 0.38742048]\n",
            " [0.20589113 0.4304672  0.34867844 0.4304672 ]\n",
            " [0.4304672  0.47829688 0.34867844 0.38742048]\n",
            " [0.20589113 0.531441   0.4304672  0.22876792]\n",
            " [0.38742048 0.25418657 0.38742048 0.18530202]\n",
            " [0.34867844 0.3138106  0.34867844 0.6561    ]\n",
            " [0.34867844 0.729      0.4304672  0.6561    ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.25418657 0.16677181 0.16677181 0.16677181]\n",
            " [0.20589113 0.20589113 0.15009463 0.22876792]\n",
            " [0.16677181 0.18530202 0.16677181 0.4304672 ]\n",
            " [0.4304672  0.47829688 0.4304672  0.531441  ]\n",
            " [0.38742048 0.47829688 0.4304672  0.531441  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.4304672  0.47829688 0.47829688 0.59049   ]\n",
            " [0.09847709 0.08862938 0.10941899 0.10941899]\n",
            " [0.10941899 0.18530202 0.10941899 0.09847709]\n",
            " [0.15009463 0.12157665 0.10941899 0.09847709]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.20589113 0.531441   0.531441   0.59049   ]\n",
            " [0.47829688 0.6561     0.531441   0.47829688]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.4304672  0.9        0.47829688 0.531441  ]\n",
            " [0.09847709 0.18530202 0.12157665 0.16677181]\n",
            " [0.16677181 0.16677181 0.12157665 0.12157665]\n",
            " [0.13508517 0.09847709 0.12157665 0.10941899]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.34867844 0.531441   0.531441   0.6561    ]\n",
            " [0.47829688 0.59049    0.59049    0.729     ]\n",
            " [0.729      0.729      0.6561     0.38742048]\n",
            " [0.6561     0.9        0.6561     0.4304672 ]\n",
            " [0.81       1.         0.729      0.9       ]\n",
            " [0.12157665 0.16677181 0.16677181 0.34867844]\n",
            " [0.18530202 0.18530202 0.15009463 0.38742048]\n",
            " [0.10941899 0.22876792 0.13508517 0.3138106 ]\n",
            " [0.25418657 0.25418657 0.22876792 0.47829688]\n",
            " [0.34867844 0.34867844 0.3138106  0.531441  ]\n",
            " [0.38742048 0.25418657 0.47829688 0.47829688]\n",
            " [0.6561     0.531441   0.531441   0.47829688]\n",
            " [0.729      0.729      0.47829688 0.729     ]\n",
            " [0.81       0.9        0.81       1.        ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "[[0.00785517 0.00872796 0.00872796 0.01077527]\n",
            " [0.01077527 0.00969774 0.00872796 0.01197252]\n",
            " [0.00706965 0.00706965 0.00872796 0.0133028 ]\n",
            " [0.0133028  0.01478088 0.00706965 0.01478088]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.01077527 0.00969774 0.01077527 0.02027556]\n",
            " [0.01077527 0.01077527 0.00969774 0.0225284 ]\n",
            " [0.00572642 0.0225284  0.00706965 0.0225284 ]\n",
            " [0.02027556 0.03433684 0.00636269 0.02027556]\n",
            " [0.00872796 0.00872796 0.00969774 0.00969774]\n",
            " [0.00969774 0.00969774 0.01077527 0.00785517]\n",
            " [0.01197252 0.01478088 0.00785517 0.01077527]\n",
            " [0.0133028  0.0164232  0.01077527 0.0164232 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.00969774 0.02503156 0.02503156 0.018248  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.03433684 0.03433684 0.03433684 0.03433684]\n",
            " [0.00872796 0.00969774 0.00872796 0.00969774]\n",
            " [0.00969774 0.00636269 0.00872796 0.01077527]\n",
            " [0.00969774 0.01197252 0.01077527 0.0133028 ]\n",
            " [0.01478088 0.01478088 0.01478088 0.018248  ]\n",
            " [0.018248   0.0133028  0.0164232  0.00636269]\n",
            " [0.00706965 0.01478088 0.018248   0.02027556]\n",
            " [0.0164232  0.0225284  0.018248   0.01478088]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.03433684 0.03433684 0.02781284 0.03815204]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.01197252 0.01197252 0.0133028  0.01478088]\n",
            " [0.0133028  0.0133028  0.01077527 0.03815204]\n",
            " [0.018248   0.01478088 0.01197252 0.04239116]\n",
            " [0.018248   0.0225284  0.0164232  0.05233477]\n",
            " [0.02027556 0.0225284  0.04239116 0.05814974]\n",
            " [0.02781284 0.05814974 0.03090315 0.03815204]\n",
            " [0.01478088 0.02503156 0.05233477 0.02027556]\n",
            " [0.03815204 0.0717898  0.0225284  0.0164232 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.02781284 0.08862938 0.05814974 0.02503156]\n",
            " [0.03815204 0.07976644 0.02781284 0.08862938]\n",
            " [0.06461082 0.0717898  0.07976644 0.03090315]\n",
            " [0.0717898  0.0717898  0.08862938 0.09847709]\n",
            " [0.09847709 0.09847709 0.08862938 0.10941899]\n",
            " [0.10941899 0.10941899 0.09847709 0.13508517]\n",
            " [0.09847709 0.10941899 0.10941899 0.10941899]\n",
            " [0.10941899 0.10941899 0.09847709 0.12157665]\n",
            " [0.09847709 0.07976644 0.10941899 0.05233477]\n",
            " [0.07976644 0.06461082 0.09847709 0.07976644]\n",
            " [0.03433684 0.08862938 0.08862938 0.07976644]\n",
            " [0.03433684 0.03090315 0.02781284 0.08862938]\n",
            " [0.02781284 0.03090315 0.07976644 0.07976644]\n",
            " [0.08862938 0.15009463 0.10941899 0.08862938]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.09847709 0.13508517 0.12157665 0.12157665]\n",
            " [0.10941899 0.12157665 0.12157665 0.12157665]\n",
            " [0.10941899 0.07976644 0.10941899 0.08862938]\n",
            " [0.08862938 0.09847709 0.10941899 0.04239116]\n",
            " [0.0717898  0.05814974 0.09847709 0.0717898 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.0717898  0.13508517 0.12157665 0.12157665]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.12157665 0.18530202 0.13508517 0.15009463]\n",
            " [0.12157665 0.10941899 0.13508517 0.13508517]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.12157665 0.13508517 0.12157665 0.15009463]\n",
            " [0.12157665 0.13508517 0.12157665 0.13508517]\n",
            " [0.13508517 0.10941899 0.13508517 0.12157665]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.59049    0.531441   0.47829688 0.47829688]\n",
            " [0.4304672  0.47829688 0.531441   0.729     ]\n",
            " [0.6561     0.729      0.47829688 0.81      ]\n",
            " [0.81       0.729      0.6561     0.9       ]\n",
            " [0.9        1.         0.81       0.9       ]\n",
            " [0.16677181 0.15009463 0.15009463 0.13508517]\n",
            " [0.13508517 0.13508517 0.13508517 0.22876792]\n",
            " [0.12157665 0.25418657 0.20589113 0.4304672 ]\n",
            " [0.38742048 0.38742048 0.25418657 0.4304672 ]\n",
            " [0.38742048 0.34867844 0.38742048 0.47829688]\n",
            " [0.47829688 0.38742048 0.47829688 0.38742048]\n",
            " [0.6561     0.729      0.47829688 0.81      ]\n",
            " [0.729      0.81       0.6561     0.9       ]\n",
            " [0.729      0.729      0.81       1.        ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "trained_a_simple = load('Trajectory - Simple - Alternating.pkl')\n",
        "print(trained_a_simple)\n",
        "\n",
        "trained_a_mid = load('Trajectory - Mid - Alternating.pkl')\n",
        "print(trained_a_mid)\n",
        "\n",
        "trained_a_hard = load('Trajectory - Hard - Alternating.pkl')\n",
        "print(trained_a_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vUP5r9NFqT1f",
        "outputId": "9e9ed37b-6192-4ded-8cb1-3703dccddbf8"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9300ac15-d762-4e68-b1a6-20fd3f6c6c7b\", \"Simple - Alternating.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c4d49559-fc74-4d1c-83dc-69439cb66071\", \"Mid - Alternating.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6240c317-1e83-4d53-bfe5-72b8a0e953e3\", \"Hard - Alternating.pkl\", 1752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f18b2b34-f25d-4d10-a10b-1339ac667801\", \"Rewards - Simple - Alternating.pkl\", 90153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_7ea4b6d9-62c5-49d0-b0ef-04529e0a17cd\", \"Rewards - Mid - Alternating.pkl\", 90153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_fce8e133-e2ce-43ee-b869-8eb0b1a20d5f\", \"Rewards - Hard - Alternating.pkl\", 85463)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2370ef0d-add1-4c59-a862-8fc0f7f30eab\", \"Time - Simple - Alternating.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_36da0eaf-29a9-4b01-b655-6e0583f8f50a\", \"Time - Mid - Alternating.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_96bd6cb8-1b9f-4e6d-8c91-2d335846a846\", \"Time - Hard - Alternating.pkl\", 21)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('Trajectory - Simple - Alternating.pkl')\n",
        "files.download('Trajectory - Mid - Alternating.pkl')\n",
        "files.download('Trajectory - Hard - Alternating.pkl')\n",
        "\n",
        "files.download('Rewards - Simple - Alternating.pkl')\n",
        "files.download('Rewards - Mid - Alternating.pkl')\n",
        "files.download('Rewards - Hard - Alternating.pkl')\n",
        "\n",
        "files.download('Time - Simple - Alternating.pkl')\n",
        "files.download('Time - Mid - Alternating.pkl')\n",
        "files.download('Time - Hard - Alternating.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhy-qxHIR45Q"
      },
      "source": [
        "---\n",
        "\n",
        "# Deep Q-Learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "36TWCaB8R8ZR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3GfUBB_Jhey"
      },
      "source": [
        "Checking availability of CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP8ku5ZJJpK_",
        "outputId": "fad5ccf1-7617-4611-8565-b91d82005d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmF91Wy1KSP4"
      },
      "source": [
        "Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xx6AEfJJKYW3"
      },
      "outputs": [],
      "source": [
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSA3fLnAlB8q"
      },
      "source": [
        "Defining the exploration-exploit function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XIhsEsI5lGFB"
      },
      "outputs": [],
      "source": [
        "def compute_action_torch(environment, epsilon, policy_net):\n",
        "\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "        return np.random.choice(range(len(environment.action_space))) # Exploration\n",
        "\n",
        "    else:\n",
        "        current_state = torch.FloatTensor(environment.grid).unsqueeze(0)\n",
        "        q = policy_net(current_state)\n",
        "        return torch.argmax(q).item() # Exploit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk0msX6Om8yX"
      },
      "source": [
        "Defining the optimizing function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sGvxmkdVm76A"
      },
      "outputs": [],
      "source": [
        "def optimize(memory, policy_net, target_net, gamma, optimizer, batch_size = 100):\n",
        "\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    batch = random.sample(memory, batch_size)\n",
        "    state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*batch)\n",
        "\n",
        "    state_batch = torch.FloatTensor(np.array(state_batch))\n",
        "    action_batch = torch.LongTensor(np.array(action_batch)).unsqueeze(1)\n",
        "    reward_batch = torch.FloatTensor(np.array(reward_batch))\n",
        "    next_state_batch = torch.FloatTensor(np.array(next_state_batch))\n",
        "    done_batch = torch.FloatTensor(np.array(done_batch))\n",
        "\n",
        "    # Compute Q-values for current states\n",
        "    q_values = policy_net(state_batch).gather(1, action_batch).squeeze()\n",
        "\n",
        "    # Compute target Q-values using the target network\n",
        "    with torch.no_grad():\n",
        "        max_next_q_values = target_net(next_state_batch).max(1)[0]\n",
        "        target_q_values = reward_batch + gamma * max_next_q_values * (1 - done_batch)\n",
        "\n",
        "    loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBmAU6Cro-Tg"
      },
      "source": [
        "Function to save the weights of the policy_net and the target_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rSC-D-bbtv3h"
      },
      "outputs": [],
      "source": [
        "def save_weights(filename, neural_net):\n",
        "    torch.save(neural_net.state_dict(), filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ogARufmHumjo"
      },
      "outputs": [],
      "source": [
        "def load_weights(filename, blank_net):\n",
        "    blank_net.load_state_dict(torch.load(filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEilwyivJrVS"
      },
      "source": [
        "Defining the Deep Q-Learning function - il faut implémenter le fait que le learning commence au bout d'une certaine itération"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "nuiIigURJqfq"
      },
      "outputs": [],
      "source": [
        "def deep_q_learning(env, memory, policy_net, target_net, optimizer, alpha=0.1, gamma=0.9,  epsilon=0.1, epsilon_decay=0.006, target_update_freq = 1000, episodes = 1000):\n",
        "\n",
        "    steps = 0\n",
        "    rewards = []\n",
        "    start_time = time.time()\n",
        "    #environments_name = ['Simple', 'Mid', 'Hard']\n",
        "\n",
        "    for e in range(episodes): #Run 1k training runs\n",
        "\n",
        "        #env = random.choice(environments)\n",
        "        #print('Environment :', environments_name[environments.index(env)])\n",
        "        state = env.reset() #Part of OpenAI where you need to reset at the start of each run\n",
        "        total_reward = 0 #Set initial reward to 0\n",
        "        step_per_episode = 0\n",
        "        time_per_episode = time.time()\n",
        "\n",
        "        while True: #Loop until done == True\n",
        "            #IF random number is less than epsilon grab the random action else grab the argument max of Q[state]\n",
        "\n",
        "            #current_state_index = env.current_pos[0] + env.current_pos[1]*env.observation_space[0] # Obtain the index of the state\n",
        "\n",
        "            current_state = np.copy(env.grid)\n",
        "\n",
        "            action = compute_action_torch(env, epsilon, policy_net) # Compute the action for the current state in function of the epsilon_greedy\n",
        "\n",
        "            posp1, new_state, reward, done = env.step(action)\n",
        "\n",
        "            #state_tp1_index = posp1[0] + posp1[1]*env.observation_space[0]\n",
        "\n",
        "            memory.append((current_state, action, reward, new_state, done))\n",
        "\n",
        "            total_reward += reward #Increment your reward\n",
        "\n",
        "            optimize(memory, policy_net, target_net, gamma, optimizer)\n",
        "\n",
        "            if steps % target_update_freq == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "            if step_per_episode >= 4000:\n",
        "                done = True\n",
        "\n",
        "            if done:\n",
        "                print(f\"Episode: {e}, Reward: {total_reward}, Steps in the episode: {step_per_episode}\", f\"Time: {time.time() - time_per_episode}\")\n",
        "                break\n",
        "\n",
        "            steps += 1\n",
        "            step_per_episode += 1\n",
        "\n",
        "\n",
        "        if epsilon>0.1:\n",
        "            epsilon *= np.exp(-epsilon_decay)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "    delta_time = start_time - time.time()\n",
        "    print(f\"Time: {delta_time}\")\n",
        "\n",
        "    save_weights('policy_net_weights_simple.pth', policy_net)\n",
        "    save_weights('target_net_weights_simple.pth', target_net)\n",
        "\n",
        "    return rewards, delta_time, steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh97o3FTvXvz"
      },
      "source": [
        "Initialization of the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ap4CE5jVvbXa"
      },
      "outputs": [],
      "source": [
        "policy_net = DeepQNetwork(STATE_SIZE[0]*STATE_SIZE[1], 150, ACTION_SIZE)\n",
        "target_net = DeepQNetwork(STATE_SIZE[0]*STATE_SIZE[1], 150, ACTION_SIZE)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "learning_rate = 1\n",
        "memory_size = 100000\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr = learning_rate)\n",
        "memory = deque(maxlen = memory_size)\n",
        "\n",
        "environments = [environment_simple, environment_mid, environment_hard]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SK1Fj59xU8V"
      },
      "source": [
        "Running of the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzag5RVNxWXk",
        "outputId": "e96d25ac-2780-49fb-d821-70e7206a9724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, Reward: 0.7, Steps in the episode: 67 Time: 0.34408140182495117\n",
            "Episode: 1, Reward: -0.5000000000000002, Steps in the episode: 252 Time: 1.252807855606079\n",
            "Episode: 2, Reward: 0.4, Steps in the episode: 128 Time: 0.6090304851531982\n",
            "Episode: 3, Reward: 0.9, Steps in the episode: 78 Time: 0.41440320014953613\n",
            "Episode: 4, Reward: -0.30000000000000004, Steps in the episode: 165 Time: 0.768040657043457\n",
            "Episode: 5, Reward: 0.8, Steps in the episode: 67 Time: 0.32953929901123047\n",
            "Episode: 6, Reward: 0.10000000000000009, Steps in the episode: 234 Time: 1.1532926559448242\n",
            "Episode: 7, Reward: 0.20000000000000007, Steps in the episode: 87 Time: 0.464993953704834\n",
            "Episode: 8, Reward: 0.7, Steps in the episode: 55 Time: 0.2763967514038086\n",
            "Episode: 9, Reward: 0.6, Steps in the episode: 25 Time: 0.13501954078674316\n",
            "Episode: 10, Reward: 0.4, Steps in the episode: 131 Time: 0.6071820259094238\n",
            "Episode: 11, Reward: -0.30000000000000004, Steps in the episode: 121 Time: 0.5975897312164307\n",
            "Episode: 12, Reward: 1.1102230246251565e-16, Steps in the episode: 152 Time: 0.7209718227386475\n",
            "Episode: 13, Reward: 0.20000000000000007, Steps in the episode: 124 Time: 0.6715703010559082\n",
            "Episode: 14, Reward: 0.5, Steps in the episode: 112 Time: 0.7669622898101807\n",
            "Episode: 15, Reward: -0.19999999999999996, Steps in the episode: 230 Time: 1.4976751804351807\n",
            "Episode: 16, Reward: 0.9, Steps in the episode: 52 Time: 0.27208638191223145\n",
            "Episode: 17, Reward: 0.7, Steps in the episode: 139 Time: 0.7125389575958252\n",
            "Episode: 18, Reward: 0.6, Steps in the episode: 55 Time: 0.27907752990722656\n",
            "Episode: 19, Reward: 0.30000000000000004, Steps in the episode: 197 Time: 1.1008615493774414\n",
            "Episode: 20, Reward: 0.8, Steps in the episode: 169 Time: 0.8178975582122803\n",
            "Episode: 21, Reward: 0.7, Steps in the episode: 53 Time: 0.2696688175201416\n",
            "Episode: 22, Reward: 0.5, Steps in the episode: 145 Time: 0.7511367797851562\n",
            "Episode: 23, Reward: -0.09999999999999987, Steps in the episode: 261 Time: 1.2405328750610352\n",
            "Episode: 24, Reward: -0.5000000000000002, Steps in the episode: 297 Time: 1.3821001052856445\n",
            "Episode: 25, Reward: 1.0, Steps in the episode: 79 Time: 0.38140249252319336\n",
            "Episode: 26, Reward: 0.4, Steps in the episode: 201 Time: 1.0088896751403809\n",
            "Episode: 27, Reward: 0.30000000000000004, Steps in the episode: 188 Time: 0.8892800807952881\n",
            "Episode: 28, Reward: 0.20000000000000007, Steps in the episode: 138 Time: 0.7195577621459961\n",
            "Episode: 29, Reward: 0.20000000000000007, Steps in the episode: 53 Time: 0.36698007583618164\n",
            "Episode: 30, Reward: -0.30000000000000004, Steps in the episode: 77 Time: 0.52504563331604\n",
            "Episode: 31, Reward: -0.7000000000000004, Steps in the episode: 356 Time: 2.0184378623962402\n",
            "Episode: 32, Reward: 0.6, Steps in the episode: 78 Time: 0.35914182662963867\n",
            "Episode: 33, Reward: -0.19999999999999996, Steps in the episode: 137 Time: 0.6407821178436279\n",
            "Episode: 34, Reward: 0.7, Steps in the episode: 24 Time: 0.11173701286315918\n",
            "Episode: 35, Reward: 0.4, Steps in the episode: 66 Time: 0.30875182151794434\n",
            "Episode: 36, Reward: 0.6, Steps in the episode: 73 Time: 0.3377721309661865\n",
            "Episode: 37, Reward: -0.40000000000000013, Steps in the episode: 106 Time: 0.49733662605285645\n",
            "Episode: 38, Reward: 0.8, Steps in the episode: 76 Time: 0.369797945022583\n",
            "Episode: 39, Reward: 1.0, Steps in the episode: 38 Time: 0.18731474876403809\n",
            "Episode: 40, Reward: -1.0000000000000004, Steps in the episode: 436 Time: 2.0119645595550537\n",
            "Episode: 41, Reward: 0.6, Steps in the episode: 59 Time: 0.2789270877838135\n",
            "Episode: 42, Reward: 0.7, Steps in the episode: 154 Time: 0.7113933563232422\n",
            "Episode: 43, Reward: 0.8, Steps in the episode: 143 Time: 0.6762921810150146\n",
            "Episode: 44, Reward: 0.4, Steps in the episode: 77 Time: 0.3610103130340576\n",
            "Episode: 45, Reward: 0.5, Steps in the episode: 96 Time: 0.45709872245788574\n",
            "Episode: 46, Reward: -0.5000000000000002, Steps in the episode: 217 Time: 0.9996848106384277\n",
            "Episode: 47, Reward: 0.5, Steps in the episode: 46 Time: 0.2236189842224121\n",
            "Episode: 48, Reward: 1.0, Steps in the episode: 92 Time: 0.4459719657897949\n",
            "Episode: 49, Reward: 0.6, Steps in the episode: 66 Time: 0.4246699810028076\n",
            "Episode: 50, Reward: 0.20000000000000007, Steps in the episode: 82 Time: 0.5446503162384033\n",
            "Episode: 51, Reward: 0.6, Steps in the episode: 59 Time: 0.4145238399505615\n",
            "Episode: 52, Reward: 0.5, Steps in the episode: 113 Time: 0.8082685470581055\n",
            "Episode: 53, Reward: 0.7, Steps in the episode: 80 Time: 0.38358163833618164\n",
            "Episode: 54, Reward: 0.8, Steps in the episode: 69 Time: 0.3251519203186035\n",
            "Episode: 55, Reward: -0.30000000000000004, Steps in the episode: 242 Time: 1.1051385402679443\n",
            "Episode: 56, Reward: 0.30000000000000004, Steps in the episode: 167 Time: 0.7702517509460449\n",
            "Episode: 57, Reward: -0.6000000000000003, Steps in the episode: 438 Time: 2.033200263977051\n",
            "Episode: 58, Reward: 0.7, Steps in the episode: 47 Time: 0.2207019329071045\n",
            "Episode: 59, Reward: 0.4, Steps in the episode: 110 Time: 0.5366148948669434\n",
            "Episode: 60, Reward: 0.4, Steps in the episode: 56 Time: 0.2729966640472412\n",
            "Episode: 61, Reward: -0.9000000000000006, Steps in the episode: 375 Time: 1.8509502410888672\n",
            "Episode: 62, Reward: -0.30000000000000004, Steps in the episode: 342 Time: 1.6358141899108887\n",
            "Episode: 63, Reward: 0.4, Steps in the episode: 82 Time: 0.40097808837890625\n",
            "Episode: 64, Reward: 0.30000000000000004, Steps in the episode: 48 Time: 0.2353682518005371\n",
            "Episode: 65, Reward: -0.09999999999999987, Steps in the episode: 154 Time: 0.9564130306243896\n",
            "Episode: 66, Reward: 0.9, Steps in the episode: 71 Time: 0.4639286994934082\n",
            "Episode: 67, Reward: -0.8000000000000005, Steps in the episode: 317 Time: 1.7791166305541992\n",
            "Episode: 68, Reward: -1.3000000000000007, Steps in the episode: 251 Time: 1.1394903659820557\n",
            "Episode: 69, Reward: 0.9, Steps in the episode: 127 Time: 0.5917019844055176\n",
            "Episode: 70, Reward: 0.4, Steps in the episode: 93 Time: 0.42928338050842285\n",
            "Episode: 71, Reward: 0.9, Steps in the episode: 115 Time: 0.5467100143432617\n",
            "Episode: 72, Reward: 0.6, Steps in the episode: 63 Time: 0.31101417541503906\n",
            "Episode: 73, Reward: 0.10000000000000009, Steps in the episode: 175 Time: 0.8110311031341553\n",
            "Episode: 74, Reward: 0.30000000000000004, Steps in the episode: 125 Time: 0.5618643760681152\n",
            "Episode: 75, Reward: 0.6, Steps in the episode: 119 Time: 0.5598189830780029\n",
            "Episode: 76, Reward: 0.8, Steps in the episode: 82 Time: 0.37264370918273926\n",
            "Episode: 77, Reward: 1.0, Steps in the episode: 49 Time: 0.23789215087890625\n",
            "Episode: 78, Reward: 0.4, Steps in the episode: 107 Time: 0.4914398193359375\n",
            "Episode: 79, Reward: 0.20000000000000007, Steps in the episode: 126 Time: 0.5836482048034668\n",
            "Episode: 80, Reward: 1.1102230246251565e-16, Steps in the episode: 79 Time: 0.3541262149810791\n",
            "Episode: 81, Reward: 0.10000000000000009, Steps in the episode: 190 Time: 0.8913784027099609\n",
            "Episode: 82, Reward: -0.6000000000000003, Steps in the episode: 125 Time: 0.5889320373535156\n",
            "Episode: 83, Reward: 0.8, Steps in the episode: 117 Time: 0.5710523128509521\n",
            "Episode: 84, Reward: -0.19999999999999996, Steps in the episode: 192 Time: 1.2554705142974854\n",
            "Episode: 85, Reward: 1.0, Steps in the episode: 46 Time: 0.3221104145050049\n",
            "Episode: 86, Reward: -0.6000000000000003, Steps in the episode: 139 Time: 0.8693981170654297\n",
            "Episode: 87, Reward: 0.9, Steps in the episode: 56 Time: 0.26543378829956055\n",
            "Episode: 88, Reward: 0.9, Steps in the episode: 208 Time: 0.9613034725189209\n",
            "Episode: 89, Reward: 0.8, Steps in the episode: 65 Time: 0.3032872676849365\n",
            "Episode: 90, Reward: -1.3000000000000007, Steps in the episode: 285 Time: 1.3021397590637207\n",
            "Episode: 91, Reward: 0.9, Steps in the episode: 96 Time: 0.44637012481689453\n",
            "Episode: 92, Reward: 0.6, Steps in the episode: 49 Time: 0.22458124160766602\n",
            "Episode: 93, Reward: 0.7, Steps in the episode: 107 Time: 0.4864778518676758\n",
            "Episode: 94, Reward: -0.19999999999999996, Steps in the episode: 85 Time: 0.4070405960083008\n",
            "Episode: 95, Reward: -0.40000000000000013, Steps in the episode: 199 Time: 0.9158403873443604\n",
            "Episode: 96, Reward: 1.0, Steps in the episode: 36 Time: 0.16541552543640137\n",
            "Episode: 97, Reward: 0.8, Steps in the episode: 66 Time: 0.30658817291259766\n",
            "Episode: 98, Reward: 0.7, Steps in the episode: 69 Time: 0.34082841873168945\n",
            "Episode: 99, Reward: 0.5, Steps in the episode: 100 Time: 0.4737570285797119\n",
            "Episode: 100, Reward: -1.2000000000000006, Steps in the episode: 275 Time: 1.2686021327972412\n",
            "Episode: 101, Reward: 0.8, Steps in the episode: 109 Time: 0.5120184421539307\n",
            "Episode: 102, Reward: 0.30000000000000004, Steps in the episode: 72 Time: 0.34914422035217285\n",
            "Episode: 103, Reward: 0.7, Steps in the episode: 51 Time: 0.24166584014892578\n",
            "Episode: 104, Reward: 0.8, Steps in the episode: 79 Time: 0.3755662441253662\n",
            "Episode: 105, Reward: 0.9, Steps in the episode: 53 Time: 0.25777578353881836\n",
            "Episode: 106, Reward: 0.6, Steps in the episode: 111 Time: 0.7037279605865479\n",
            "Episode: 107, Reward: -0.40000000000000013, Steps in the episode: 146 Time: 1.0064945220947266\n",
            "Episode: 108, Reward: 0.7, Steps in the episode: 204 Time: 1.1982758045196533\n",
            "Episode: 109, Reward: 0.8, Steps in the episode: 59 Time: 0.27770209312438965\n",
            "Episode: 110, Reward: 0.8, Steps in the episode: 33 Time: 0.16173601150512695\n",
            "Episode: 111, Reward: 0.6, Steps in the episode: 56 Time: 0.2646598815917969\n",
            "Episode: 112, Reward: 0.20000000000000007, Steps in the episode: 179 Time: 0.8283431529998779\n",
            "Episode: 113, Reward: 0.8, Steps in the episode: 96 Time: 0.4381883144378662\n",
            "Episode: 114, Reward: 1.1102230246251565e-16, Steps in the episode: 88 Time: 0.41044020652770996\n",
            "Episode: 115, Reward: 0.8, Steps in the episode: 75 Time: 0.36557936668395996\n",
            "Episode: 116, Reward: 1.0, Steps in the episode: 59 Time: 0.27298736572265625\n",
            "Episode: 117, Reward: 1.0, Steps in the episode: 63 Time: 0.30938053131103516\n",
            "Episode: 118, Reward: 0.20000000000000007, Steps in the episode: 213 Time: 0.9849801063537598\n",
            "Episode: 119, Reward: 0.5, Steps in the episode: 125 Time: 0.563713788986206\n",
            "Episode: 120, Reward: 0.8, Steps in the episode: 101 Time: 0.4822521209716797\n",
            "Episode: 121, Reward: 0.4, Steps in the episode: 103 Time: 0.4643845558166504\n",
            "Episode: 122, Reward: 0.4, Steps in the episode: 74 Time: 0.3507392406463623\n",
            "Episode: 123, Reward: 0.7, Steps in the episode: 54 Time: 0.25223755836486816\n",
            "Episode: 124, Reward: 0.8, Steps in the episode: 47 Time: 0.2289259433746338\n",
            "Episode: 125, Reward: 0.6, Steps in the episode: 115 Time: 0.5489201545715332\n",
            "Episode: 126, Reward: -0.40000000000000013, Steps in the episode: 135 Time: 0.6114141941070557\n",
            "Episode: 127, Reward: 0.5, Steps in the episode: 69 Time: 0.3130629062652588\n",
            "Episode: 128, Reward: 0.30000000000000004, Steps in the episode: 92 Time: 0.4460315704345703\n",
            "Episode: 129, Reward: 0.6, Steps in the episode: 47 Time: 0.2234635353088379\n",
            "Episode: 130, Reward: 0.10000000000000009, Steps in the episode: 150 Time: 0.7945170402526855\n",
            "Episode: 131, Reward: 0.20000000000000007, Steps in the episode: 72 Time: 0.496488094329834\n",
            "Episode: 132, Reward: 0.8, Steps in the episode: 165 Time: 1.1717121601104736\n",
            "Episode: 133, Reward: -0.5000000000000002, Steps in the episode: 344 Time: 1.694014549255371\n",
            "Episode: 134, Reward: 0.5, Steps in the episode: 128 Time: 0.5960121154785156\n",
            "Episode: 135, Reward: -0.30000000000000004, Steps in the episode: 292 Time: 1.3580560684204102\n",
            "Episode: 136, Reward: 0.7, Steps in the episode: 99 Time: 0.47177863121032715\n",
            "Episode: 137, Reward: 1.1102230246251565e-16, Steps in the episode: 94 Time: 0.43732738494873047\n",
            "Episode: 138, Reward: 0.4, Steps in the episode: 79 Time: 0.358870267868042\n",
            "Episode: 139, Reward: 0.7, Steps in the episode: 103 Time: 0.480363130569458\n",
            "Episode: 140, Reward: 0.4, Steps in the episode: 173 Time: 0.7910470962524414\n",
            "Episode: 141, Reward: 0.8, Steps in the episode: 73 Time: 0.3338956832885742\n",
            "Episode: 142, Reward: -0.7000000000000004, Steps in the episode: 153 Time: 0.7093920707702637\n",
            "Episode: 143, Reward: 0.6, Steps in the episode: 74 Time: 0.3438720703125\n",
            "Episode: 144, Reward: 0.8, Steps in the episode: 102 Time: 0.47573256492614746\n",
            "Episode: 145, Reward: 0.9, Steps in the episode: 133 Time: 0.6253130435943604\n",
            "Episode: 146, Reward: 0.5, Steps in the episode: 78 Time: 0.3652973175048828\n",
            "Episode: 147, Reward: -0.19999999999999996, Steps in the episode: 167 Time: 0.8095171451568604\n",
            "Episode: 148, Reward: 1.0, Steps in the episode: 70 Time: 0.3262345790863037\n",
            "Episode: 149, Reward: 0.20000000000000007, Steps in the episode: 252 Time: 1.669706106185913\n",
            "Episode: 150, Reward: 0.9, Steps in the episode: 31 Time: 0.22526764869689941\n",
            "Episode: 151, Reward: 0.8, Steps in the episode: 63 Time: 0.42499804496765137\n",
            "Episode: 152, Reward: 0.4, Steps in the episode: 103 Time: 0.4801337718963623\n",
            "Episode: 153, Reward: 0.4, Steps in the episode: 80 Time: 0.3712277412414551\n",
            "Episode: 154, Reward: 0.7, Steps in the episode: 80 Time: 0.3810138702392578\n",
            "Episode: 155, Reward: 0.6, Steps in the episode: 147 Time: 0.6974341869354248\n",
            "Episode: 156, Reward: 0.8, Steps in the episode: 53 Time: 0.24580669403076172\n",
            "Episode: 157, Reward: 0.20000000000000007, Steps in the episode: 201 Time: 0.9413063526153564\n",
            "Episode: 158, Reward: -1.3000000000000007, Steps in the episode: 403 Time: 1.8561739921569824\n",
            "Episode: 159, Reward: 0.8, Steps in the episode: 209 Time: 0.9682204723358154\n",
            "Episode: 160, Reward: 0.10000000000000009, Steps in the episode: 107 Time: 0.5111830234527588\n",
            "Episode: 161, Reward: 0.5, Steps in the episode: 85 Time: 0.39246058464050293\n",
            "Episode: 162, Reward: 0.5, Steps in the episode: 164 Time: 0.7580156326293945\n",
            "Episode: 163, Reward: 0.6, Steps in the episode: 130 Time: 0.6114101409912109\n",
            "Episode: 164, Reward: 0.4, Steps in the episode: 139 Time: 0.633774995803833\n",
            "Episode: 165, Reward: 0.7, Steps in the episode: 143 Time: 0.6837916374206543\n",
            "Episode: 166, Reward: 0.4, Steps in the episode: 126 Time: 0.6986546516418457\n",
            "Episode: 167, Reward: 0.6, Steps in the episode: 43 Time: 0.30678629875183105\n",
            "Episode: 168, Reward: 1.1102230246251565e-16, Steps in the episode: 133 Time: 0.9235754013061523\n",
            "Episode: 169, Reward: -0.19999999999999996, Steps in the episode: 119 Time: 0.8314011096954346\n",
            "Episode: 170, Reward: -0.8000000000000005, Steps in the episode: 100 Time: 0.47335076332092285\n",
            "Episode: 171, Reward: -0.30000000000000004, Steps in the episode: 101 Time: 0.4773390293121338\n",
            "Episode: 172, Reward: -1.3000000000000007, Steps in the episode: 489 Time: 2.263481378555298\n",
            "Episode: 173, Reward: -0.5000000000000002, Steps in the episode: 372 Time: 1.6964242458343506\n",
            "Episode: 174, Reward: 0.9, Steps in the episode: 23 Time: 0.10756802558898926\n",
            "Episode: 175, Reward: 0.6, Steps in the episode: 89 Time: 0.41663455963134766\n",
            "Episode: 176, Reward: 0.4, Steps in the episode: 86 Time: 0.40228843688964844\n",
            "Episode: 177, Reward: 0.10000000000000009, Steps in the episode: 166 Time: 0.7812545299530029\n",
            "Episode: 178, Reward: 0.20000000000000007, Steps in the episode: 177 Time: 0.8332324028015137\n",
            "Episode: 179, Reward: -0.09999999999999987, Steps in the episode: 173 Time: 0.8090798854827881\n",
            "Episode: 180, Reward: 0.7, Steps in the episode: 71 Time: 0.33396029472351074\n",
            "Episode: 181, Reward: -0.09999999999999987, Steps in the episode: 514 Time: 2.9354445934295654\n",
            "Episode: 182, Reward: 0.8, Steps in the episode: 71 Time: 0.5250344276428223\n",
            "Episode: 183, Reward: -1.600000000000001, Steps in the episode: 573 Time: 2.6496903896331787\n",
            "Episode: 184, Reward: 0.5, Steps in the episode: 78 Time: 0.3629171848297119\n",
            "Episode: 185, Reward: 0.8, Steps in the episode: 85 Time: 0.403423547744751\n",
            "Episode: 186, Reward: 0.10000000000000009, Steps in the episode: 162 Time: 0.7546639442443848\n",
            "Episode: 187, Reward: 0.20000000000000007, Steps in the episode: 99 Time: 0.47104573249816895\n",
            "Episode: 188, Reward: -0.40000000000000013, Steps in the episode: 225 Time: 1.0842974185943604\n",
            "Episode: 189, Reward: 0.5, Steps in the episode: 265 Time: 1.2397305965423584\n",
            "Episode: 190, Reward: 0.6, Steps in the episode: 78 Time: 0.3673536777496338\n",
            "Episode: 191, Reward: 0.10000000000000009, Steps in the episode: 122 Time: 0.5769383907318115\n",
            "Episode: 192, Reward: 0.30000000000000004, Steps in the episode: 150 Time: 0.7135233879089355\n",
            "Episode: 193, Reward: -1.700000000000001, Steps in the episode: 442 Time: 2.4099907875061035\n",
            "Episode: 194, Reward: 0.8, Steps in the episode: 32 Time: 0.2420954704284668\n",
            "Episode: 195, Reward: -1.600000000000001, Steps in the episode: 394 Time: 2.232020854949951\n",
            "Episode: 196, Reward: -0.40000000000000013, Steps in the episode: 268 Time: 1.2849669456481934\n",
            "Episode: 197, Reward: 0.20000000000000007, Steps in the episode: 104 Time: 0.49129509925842285\n",
            "Episode: 198, Reward: 0.8, Steps in the episode: 58 Time: 0.27945828437805176\n",
            "Episode: 199, Reward: 0.7, Steps in the episode: 52 Time: 0.2655024528503418\n",
            "Episode: 200, Reward: 0.5, Steps in the episode: 52 Time: 0.2654900550842285\n",
            "Episode: 201, Reward: 0.8, Steps in the episode: 116 Time: 0.5594029426574707\n",
            "Episode: 202, Reward: 0.8, Steps in the episode: 78 Time: 0.3841686248779297\n",
            "Episode: 203, Reward: 0.5, Steps in the episode: 94 Time: 0.4619920253753662\n",
            "Episode: 204, Reward: -1.0000000000000004, Steps in the episode: 335 Time: 1.6257729530334473\n",
            "Episode: 205, Reward: 0.20000000000000007, Steps in the episode: 122 Time: 0.5761940479278564\n",
            "Episode: 206, Reward: -0.30000000000000004, Steps in the episode: 157 Time: 0.7756946086883545\n",
            "Episode: 207, Reward: -1.4000000000000008, Steps in the episode: 275 Time: 1.3328518867492676\n",
            "Episode: 208, Reward: 0.10000000000000009, Steps in the episode: 241 Time: 1.497168779373169\n",
            "Episode: 209, Reward: 0.5, Steps in the episode: 78 Time: 0.5470163822174072\n",
            "Episode: 210, Reward: 1.0, Steps in the episode: 55 Time: 0.4289591312408447\n",
            "Episode: 211, Reward: -0.9000000000000006, Steps in the episode: 222 Time: 1.1799595355987549\n",
            "Episode: 212, Reward: -1.600000000000001, Steps in the episode: 322 Time: 1.529242753982544\n",
            "Episode: 213, Reward: 0.5, Steps in the episode: 39 Time: 0.20643997192382812\n",
            "Episode: 214, Reward: 0.8, Steps in the episode: 74 Time: 0.35478758811950684\n",
            "Episode: 215, Reward: 0.30000000000000004, Steps in the episode: 122 Time: 0.6042821407318115\n",
            "Episode: 216, Reward: 0.6, Steps in the episode: 116 Time: 0.5606749057769775\n",
            "Episode: 217, Reward: -0.09999999999999987, Steps in the episode: 134 Time: 0.6453418731689453\n",
            "Episode: 218, Reward: 1.0, Steps in the episode: 65 Time: 0.31520605087280273\n",
            "Episode: 219, Reward: -1.4000000000000008, Steps in the episode: 305 Time: 1.4666714668273926\n",
            "Episode: 220, Reward: 0.9, Steps in the episode: 56 Time: 0.2704465389251709\n",
            "Episode: 221, Reward: 0.8, Steps in the episode: 203 Time: 0.9505693912506104\n",
            "Episode: 222, Reward: -0.09999999999999987, Steps in the episode: 298 Time: 1.4228403568267822\n",
            "Episode: 223, Reward: 1.1102230246251565e-16, Steps in the episode: 120 Time: 0.5863156318664551\n",
            "Episode: 224, Reward: 0.8, Steps in the episode: 80 Time: 0.5080053806304932\n",
            "Episode: 225, Reward: 0.8, Steps in the episode: 45 Time: 0.32379841804504395\n",
            "Episode: 226, Reward: 0.30000000000000004, Steps in the episode: 158 Time: 1.1149230003356934\n",
            "Episode: 227, Reward: 0.6, Steps in the episode: 69 Time: 0.5172200202941895\n",
            "Episode: 228, Reward: -0.09999999999999987, Steps in the episode: 130 Time: 0.6283788681030273\n",
            "Episode: 229, Reward: -0.19999999999999996, Steps in the episode: 136 Time: 0.629305362701416\n",
            "Episode: 230, Reward: 0.9, Steps in the episode: 177 Time: 0.8424856662750244\n",
            "Episode: 231, Reward: 0.7, Steps in the episode: 63 Time: 0.31256532669067383\n",
            "Episode: 232, Reward: 1.1102230246251565e-16, Steps in the episode: 74 Time: 0.3808588981628418\n",
            "Episode: 233, Reward: 0.7, Steps in the episode: 60 Time: 0.30059027671813965\n",
            "Episode: 234, Reward: -0.9000000000000006, Steps in the episode: 169 Time: 0.8194525241851807\n",
            "Episode: 235, Reward: 0.6, Steps in the episode: 77 Time: 0.3729066848754883\n",
            "Episode: 236, Reward: 1.1102230246251565e-16, Steps in the episode: 243 Time: 1.1895668506622314\n",
            "Episode: 237, Reward: 0.4, Steps in the episode: 53 Time: 0.26983070373535156\n",
            "Episode: 238, Reward: 0.30000000000000004, Steps in the episode: 96 Time: 0.4718046188354492\n",
            "Episode: 239, Reward: 1.1102230246251565e-16, Steps in the episode: 314 Time: 1.4820764064788818\n",
            "Episode: 240, Reward: 0.7, Steps in the episode: 46 Time: 0.2287001609802246\n",
            "Episode: 241, Reward: -0.40000000000000013, Steps in the episode: 94 Time: 0.4588329792022705\n",
            "Episode: 242, Reward: 0.7, Steps in the episode: 95 Time: 0.46802258491516113\n",
            "Episode: 243, Reward: 1.0, Steps in the episode: 33 Time: 0.16128802299499512\n",
            "Episode: 244, Reward: 0.8, Steps in the episode: 45 Time: 0.21999382972717285\n",
            "Episode: 245, Reward: 0.9, Steps in the episode: 92 Time: 0.44118213653564453\n",
            "Episode: 246, Reward: 0.20000000000000007, Steps in the episode: 126 Time: 0.7702193260192871\n",
            "Episode: 247, Reward: -0.19999999999999996, Steps in the episode: 239 Time: 1.7138078212738037\n",
            "Episode: 248, Reward: 0.4, Steps in the episode: 97 Time: 0.5133581161499023\n",
            "Episode: 249, Reward: 0.20000000000000007, Steps in the episode: 68 Time: 0.341066837310791\n",
            "Episode: 250, Reward: 0.7, Steps in the episode: 34 Time: 0.17960286140441895\n",
            "Episode: 251, Reward: 1.0, Steps in the episode: 32 Time: 0.16005873680114746\n",
            "Episode: 252, Reward: 0.8, Steps in the episode: 111 Time: 0.5391762256622314\n",
            "Episode: 253, Reward: 1.1102230246251565e-16, Steps in the episode: 143 Time: 0.7230675220489502\n",
            "Episode: 254, Reward: 0.6, Steps in the episode: 130 Time: 0.6362259387969971\n",
            "Episode: 255, Reward: 0.30000000000000004, Steps in the episode: 96 Time: 0.47025156021118164\n",
            "Episode: 256, Reward: 0.7, Steps in the episode: 45 Time: 0.2148301601409912\n",
            "Episode: 257, Reward: 0.8, Steps in the episode: 171 Time: 0.8273425102233887\n",
            "Episode: 258, Reward: 0.6, Steps in the episode: 144 Time: 0.6751976013183594\n",
            "Episode: 259, Reward: -1.1000000000000005, Steps in the episode: 216 Time: 0.9887773990631104\n",
            "Episode: 260, Reward: 1.0, Steps in the episode: 62 Time: 0.28667211532592773\n",
            "Episode: 261, Reward: 0.30000000000000004, Steps in the episode: 45 Time: 0.21447372436523438\n",
            "Episode: 262, Reward: 0.8, Steps in the episode: 54 Time: 0.2651238441467285\n",
            "Episode: 263, Reward: 0.7, Steps in the episode: 52 Time: 0.24342060089111328\n",
            "Episode: 264, Reward: 0.20000000000000007, Steps in the episode: 83 Time: 0.3981034755706787\n",
            "Episode: 265, Reward: 0.9, Steps in the episode: 88 Time: 0.4239199161529541\n",
            "Episode: 266, Reward: 0.4, Steps in the episode: 194 Time: 0.9030261039733887\n",
            "Episode: 267, Reward: 0.5, Steps in the episode: 42 Time: 0.20041990280151367\n",
            "Episode: 268, Reward: 0.6, Steps in the episode: 76 Time: 0.3549003601074219\n",
            "Episode: 269, Reward: 0.6, Steps in the episode: 69 Time: 0.32056641578674316\n",
            "Episode: 270, Reward: -0.19999999999999996, Steps in the episode: 177 Time: 1.136967658996582\n",
            "Episode: 271, Reward: 0.9, Steps in the episode: 91 Time: 0.6445901393890381\n",
            "Episode: 272, Reward: 0.4, Steps in the episode: 143 Time: 0.8925285339355469\n",
            "Episode: 273, Reward: 0.4, Steps in the episode: 78 Time: 0.38073277473449707\n",
            "Episode: 274, Reward: 0.9, Steps in the episode: 43 Time: 0.22460007667541504\n",
            "Episode: 275, Reward: 0.7, Steps in the episode: 107 Time: 0.5096912384033203\n",
            "Episode: 276, Reward: 1.1102230246251565e-16, Steps in the episode: 83 Time: 0.38663220405578613\n",
            "Episode: 277, Reward: 1.1102230246251565e-16, Steps in the episode: 115 Time: 0.5422172546386719\n",
            "Episode: 278, Reward: 0.9, Steps in the episode: 110 Time: 0.5076053142547607\n",
            "Episode: 279, Reward: -0.9000000000000006, Steps in the episode: 114 Time: 0.5516283512115479\n",
            "Episode: 280, Reward: -0.30000000000000004, Steps in the episode: 134 Time: 0.6402058601379395\n",
            "Episode: 281, Reward: 0.7, Steps in the episode: 81 Time: 0.3854224681854248\n",
            "Episode: 282, Reward: 0.5, Steps in the episode: 75 Time: 0.35807085037231445\n",
            "Episode: 283, Reward: 0.4, Steps in the episode: 64 Time: 0.3125143051147461\n",
            "Episode: 284, Reward: -0.19999999999999996, Steps in the episode: 154 Time: 0.7121596336364746\n",
            "Episode: 285, Reward: 0.7, Steps in the episode: 38 Time: 0.18324899673461914\n",
            "Episode: 286, Reward: 0.5, Steps in the episode: 76 Time: 0.35543203353881836\n",
            "Episode: 287, Reward: 0.5, Steps in the episode: 184 Time: 0.8593623638153076\n",
            "Episode: 288, Reward: 0.20000000000000007, Steps in the episode: 112 Time: 0.5299768447875977\n",
            "Episode: 289, Reward: 0.4, Steps in the episode: 161 Time: 0.7514936923980713\n",
            "Episode: 290, Reward: 0.9, Steps in the episode: 71 Time: 0.3516249656677246\n",
            "Episode: 291, Reward: 0.10000000000000009, Steps in the episode: 85 Time: 0.409409761428833\n",
            "Episode: 292, Reward: 1.0, Steps in the episode: 57 Time: 0.26116943359375\n",
            "Episode: 293, Reward: 0.10000000000000009, Steps in the episode: 101 Time: 0.47329187393188477\n",
            "Episode: 294, Reward: 0.4, Steps in the episode: 90 Time: 0.6278836727142334\n",
            "Episode: 295, Reward: -0.40000000000000013, Steps in the episode: 339 Time: 2.100958824157715\n",
            "Episode: 296, Reward: 0.6, Steps in the episode: 159 Time: 0.7673618793487549\n",
            "Episode: 297, Reward: 0.6, Steps in the episode: 66 Time: 0.3179476261138916\n",
            "Episode: 298, Reward: 0.7, Steps in the episode: 71 Time: 0.33400583267211914\n",
            "Episode: 299, Reward: -0.30000000000000004, Steps in the episode: 146 Time: 0.697251558303833\n",
            "Episode: 300, Reward: -0.7000000000000004, Steps in the episode: 196 Time: 0.9271342754364014\n",
            "Episode: 301, Reward: 0.20000000000000007, Steps in the episode: 55 Time: 0.27286553382873535\n",
            "Episode: 302, Reward: 0.8, Steps in the episode: 48 Time: 0.2359151840209961\n",
            "Episode: 303, Reward: 0.8, Steps in the episode: 87 Time: 0.42629098892211914\n",
            "Episode: 304, Reward: 0.8, Steps in the episode: 47 Time: 0.22978949546813965\n",
            "Episode: 305, Reward: -0.19999999999999996, Steps in the episode: 70 Time: 0.3397364616394043\n",
            "Episode: 306, Reward: -0.40000000000000013, Steps in the episode: 360 Time: 1.6991491317749023\n",
            "Episode: 307, Reward: -1.3000000000000007, Steps in the episode: 248 Time: 1.1965928077697754\n",
            "Episode: 308, Reward: 0.4, Steps in the episode: 55 Time: 0.27117490768432617\n",
            "Episode: 309, Reward: 0.30000000000000004, Steps in the episode: 135 Time: 0.6441888809204102\n",
            "Episode: 310, Reward: 0.4, Steps in the episode: 173 Time: 0.8356368541717529\n",
            "Episode: 311, Reward: 0.5, Steps in the episode: 165 Time: 1.0551621913909912\n",
            "Episode: 312, Reward: 0.9, Steps in the episode: 39 Time: 0.28002476692199707\n",
            "Episode: 313, Reward: -0.8000000000000005, Steps in the episode: 306 Time: 1.8551783561706543\n",
            "Episode: 314, Reward: 0.8, Steps in the episode: 38 Time: 0.18837690353393555\n",
            "Episode: 315, Reward: 0.8, Steps in the episode: 42 Time: 0.21518945693969727\n",
            "Episode: 316, Reward: 0.9, Steps in the episode: 69 Time: 0.34485673904418945\n",
            "Episode: 317, Reward: 1.1102230246251565e-16, Steps in the episode: 109 Time: 0.5252211093902588\n",
            "Episode: 318, Reward: 0.8, Steps in the episode: 83 Time: 0.41275858879089355\n",
            "Episode: 319, Reward: 0.6, Steps in the episode: 80 Time: 0.38176774978637695\n",
            "Episode: 320, Reward: 0.4, Steps in the episode: 152 Time: 0.7149953842163086\n",
            "Episode: 321, Reward: 0.9, Steps in the episode: 143 Time: 0.6700882911682129\n",
            "Episode: 322, Reward: 0.7, Steps in the episode: 39 Time: 0.19113802909851074\n",
            "Episode: 323, Reward: 0.10000000000000009, Steps in the episode: 68 Time: 0.32193684577941895\n",
            "Episode: 324, Reward: 1.0, Steps in the episode: 27 Time: 0.13492965698242188\n",
            "Episode: 325, Reward: 0.4, Steps in the episode: 184 Time: 0.8799726963043213\n",
            "Episode: 326, Reward: 0.7, Steps in the episode: 46 Time: 0.22574877738952637\n",
            "Episode: 327, Reward: 0.9, Steps in the episode: 49 Time: 0.23705840110778809\n",
            "Episode: 328, Reward: -0.19999999999999996, Steps in the episode: 165 Time: 0.7811172008514404\n",
            "Episode: 329, Reward: 0.4, Steps in the episode: 39 Time: 0.1841566562652588\n",
            "Episode: 330, Reward: 0.5, Steps in the episode: 106 Time: 0.4967525005340576\n",
            "Episode: 331, Reward: 0.6, Steps in the episode: 70 Time: 0.3277120590209961\n",
            "Episode: 332, Reward: 0.10000000000000009, Steps in the episode: 208 Time: 0.9696474075317383\n",
            "Episode: 333, Reward: 1.0, Steps in the episode: 48 Time: 0.22791600227355957\n",
            "Episode: 334, Reward: 0.8, Steps in the episode: 57 Time: 0.2822268009185791\n",
            "Episode: 335, Reward: 0.8, Steps in the episode: 57 Time: 0.266538143157959\n",
            "Episode: 336, Reward: 0.6, Steps in the episode: 73 Time: 0.4093661308288574\n",
            "Episode: 337, Reward: 0.6, Steps in the episode: 49 Time: 0.367464542388916\n",
            "Episode: 338, Reward: 0.9, Steps in the episode: 25 Time: 0.18132519721984863\n",
            "Episode: 339, Reward: 0.7, Steps in the episode: 77 Time: 0.5331165790557861\n",
            "Episode: 340, Reward: 0.8, Steps in the episode: 38 Time: 0.2798755168914795\n",
            "Episode: 341, Reward: 1.0, Steps in the episode: 403 Time: 2.1575942039489746\n",
            "Episode: 342, Reward: 0.8, Steps in the episode: 144 Time: 0.6923537254333496\n",
            "Episode: 343, Reward: 0.30000000000000004, Steps in the episode: 107 Time: 0.5298480987548828\n",
            "Episode: 344, Reward: 0.4, Steps in the episode: 67 Time: 0.3159046173095703\n",
            "Episode: 345, Reward: -0.5000000000000002, Steps in the episode: 187 Time: 0.8911135196685791\n",
            "Episode: 346, Reward: 0.10000000000000009, Steps in the episode: 103 Time: 0.48774290084838867\n",
            "Episode: 347, Reward: 0.8, Steps in the episode: 46 Time: 0.23624753952026367\n",
            "Episode: 348, Reward: 0.30000000000000004, Steps in the episode: 364 Time: 1.705554485321045\n",
            "Episode: 349, Reward: -0.7000000000000004, Steps in the episode: 144 Time: 0.6944715976715088\n",
            "Episode: 350, Reward: 0.4, Steps in the episode: 105 Time: 0.4973926544189453\n",
            "Episode: 351, Reward: 0.7, Steps in the episode: 61 Time: 0.29929590225219727\n",
            "Episode: 352, Reward: 0.30000000000000004, Steps in the episode: 55 Time: 0.2688412666320801\n",
            "Episode: 353, Reward: 0.4, Steps in the episode: 99 Time: 0.47545433044433594\n",
            "Episode: 354, Reward: 0.30000000000000004, Steps in the episode: 27 Time: 0.14107894897460938\n",
            "Episode: 355, Reward: 0.9, Steps in the episode: 62 Time: 0.29782772064208984\n",
            "Episode: 356, Reward: 0.9, Steps in the episode: 113 Time: 0.5331401824951172\n",
            "Episode: 357, Reward: 1.1102230246251565e-16, Steps in the episode: 104 Time: 0.5484473705291748\n",
            "Episode: 358, Reward: 0.6, Steps in the episode: 37 Time: 0.26412534713745117\n",
            "Episode: 359, Reward: 0.8, Steps in the episode: 82 Time: 0.5631730556488037\n",
            "Episode: 360, Reward: -0.40000000000000013, Steps in the episode: 218 Time: 1.3947646617889404\n",
            "Episode: 361, Reward: 0.4, Steps in the episode: 50 Time: 0.23408722877502441\n",
            "Episode: 362, Reward: 1.0, Steps in the episode: 42 Time: 0.2207808494567871\n",
            "Episode: 363, Reward: -0.19999999999999996, Steps in the episode: 211 Time: 1.0230515003204346\n",
            "Episode: 364, Reward: 0.5, Steps in the episode: 96 Time: 0.45667576789855957\n",
            "Episode: 365, Reward: 0.8, Steps in the episode: 75 Time: 0.3603842258453369\n",
            "Episode: 366, Reward: 0.9, Steps in the episode: 63 Time: 0.3110377788543701\n",
            "Episode: 367, Reward: -0.40000000000000013, Steps in the episode: 89 Time: 0.4373037815093994\n",
            "Episode: 368, Reward: 0.10000000000000009, Steps in the episode: 136 Time: 0.6497001647949219\n",
            "Episode: 369, Reward: 0.9, Steps in the episode: 22 Time: 0.10676240921020508\n",
            "Episode: 370, Reward: 1.1102230246251565e-16, Steps in the episode: 137 Time: 0.6557478904724121\n",
            "Episode: 371, Reward: -0.40000000000000013, Steps in the episode: 248 Time: 1.164520025253296\n",
            "Episode: 372, Reward: 0.6, Steps in the episode: 87 Time: 0.40392589569091797\n",
            "Episode: 373, Reward: 0.10000000000000009, Steps in the episode: 152 Time: 0.7555685043334961\n",
            "Episode: 374, Reward: 0.8, Steps in the episode: 67 Time: 0.31305503845214844\n",
            "Episode: 375, Reward: 0.9, Steps in the episode: 39 Time: 0.18848609924316406\n",
            "Episode: 376, Reward: 0.9, Steps in the episode: 71 Time: 0.34233927726745605\n",
            "Episode: 377, Reward: 0.8, Steps in the episode: 37 Time: 0.1755223274230957\n",
            "Episode: 378, Reward: -0.9000000000000006, Steps in the episode: 211 Time: 0.9819037914276123\n",
            "Episode: 379, Reward: 0.20000000000000007, Steps in the episode: 62 Time: 0.29979515075683594\n",
            "Episode: 380, Reward: 0.7, Steps in the episode: 198 Time: 1.0485713481903076\n",
            "Episode: 381, Reward: 0.5, Steps in the episode: 107 Time: 0.7401127815246582\n",
            "Episode: 382, Reward: 0.4, Steps in the episode: 202 Time: 1.2761821746826172\n",
            "Episode: 383, Reward: 0.4, Steps in the episode: 44 Time: 0.21182632446289062\n",
            "Episode: 384, Reward: 0.6, Steps in the episode: 143 Time: 0.6824386119842529\n",
            "Episode: 385, Reward: 1.1102230246251565e-16, Steps in the episode: 156 Time: 0.7517602443695068\n",
            "Episode: 386, Reward: 0.7, Steps in the episode: 62 Time: 0.2956881523132324\n",
            "Episode: 387, Reward: 1.1102230246251565e-16, Steps in the episode: 82 Time: 0.40127086639404297\n",
            "Episode: 388, Reward: 0.10000000000000009, Steps in the episode: 152 Time: 0.7409865856170654\n",
            "Episode: 389, Reward: 0.9, Steps in the episode: 31 Time: 0.1536104679107666\n",
            "Episode: 390, Reward: 0.7, Steps in the episode: 240 Time: 1.14560866355896\n",
            "Episode: 391, Reward: 1.0, Steps in the episode: 50 Time: 0.23808956146240234\n",
            "Episode: 392, Reward: 0.30000000000000004, Steps in the episode: 155 Time: 0.744748592376709\n",
            "Episode: 393, Reward: -0.09999999999999987, Steps in the episode: 252 Time: 1.1635675430297852\n",
            "Episode: 394, Reward: 0.30000000000000004, Steps in the episode: 119 Time: 0.5591373443603516\n",
            "Episode: 395, Reward: -0.7000000000000004, Steps in the episode: 239 Time: 1.1306426525115967\n",
            "Episode: 396, Reward: -1.5000000000000009, Steps in the episode: 200 Time: 0.9356579780578613\n",
            "Episode: 397, Reward: 0.20000000000000007, Steps in the episode: 170 Time: 0.9055123329162598\n",
            "Episode: 398, Reward: 0.20000000000000007, Steps in the episode: 106 Time: 0.7177720069885254\n",
            "Episode: 399, Reward: 0.7, Steps in the episode: 64 Time: 0.4579508304595947\n",
            "Episode: 400, Reward: 1.0, Steps in the episode: 48 Time: 0.35084104537963867\n",
            "Episode: 401, Reward: 0.6, Steps in the episode: 91 Time: 0.49715590476989746\n",
            "Episode: 402, Reward: 0.10000000000000009, Steps in the episode: 145 Time: 0.675513744354248\n",
            "Episode: 403, Reward: -0.09999999999999987, Steps in the episode: 390 Time: 1.8319416046142578\n",
            "Episode: 404, Reward: 0.6, Steps in the episode: 146 Time: 0.6893770694732666\n",
            "Episode: 405, Reward: 0.10000000000000009, Steps in the episode: 167 Time: 0.8100512027740479\n",
            "Episode: 406, Reward: 0.8, Steps in the episode: 34 Time: 0.16898655891418457\n",
            "Episode: 407, Reward: 0.30000000000000004, Steps in the episode: 38 Time: 0.188460111618042\n",
            "Episode: 408, Reward: 0.7, Steps in the episode: 64 Time: 0.31122398376464844\n",
            "Episode: 409, Reward: 0.6, Steps in the episode: 118 Time: 0.560356616973877\n",
            "Episode: 410, Reward: 0.7, Steps in the episode: 131 Time: 0.6157214641571045\n",
            "Episode: 411, Reward: 0.4, Steps in the episode: 233 Time: 1.0821566581726074\n",
            "Episode: 412, Reward: 0.5, Steps in the episode: 154 Time: 0.741706371307373\n",
            "Episode: 413, Reward: 0.9, Steps in the episode: 81 Time: 0.37517428398132324\n",
            "Episode: 414, Reward: 0.5, Steps in the episode: 60 Time: 0.2958099842071533\n",
            "Episode: 415, Reward: 0.8, Steps in the episode: 113 Time: 0.5262503623962402\n",
            "Episode: 416, Reward: 0.6, Steps in the episode: 102 Time: 0.4897432327270508\n",
            "Episode: 417, Reward: 0.8, Steps in the episode: 26 Time: 0.12723112106323242\n",
            "Episode: 418, Reward: -0.7000000000000004, Steps in the episode: 132 Time: 0.8495090007781982\n",
            "Episode: 419, Reward: 0.7, Steps in the episode: 46 Time: 0.33176660537719727\n",
            "Episode: 420, Reward: 0.5, Steps in the episode: 94 Time: 0.6488831043243408\n",
            "Episode: 421, Reward: 0.9, Steps in the episode: 89 Time: 0.6075830459594727\n",
            "Episode: 422, Reward: 1.0, Steps in the episode: 36 Time: 0.17457175254821777\n",
            "Episode: 423, Reward: -0.19999999999999996, Steps in the episode: 140 Time: 0.6558279991149902\n",
            "Episode: 424, Reward: 1.0, Steps in the episode: 56 Time: 0.25760412216186523\n",
            "Episode: 425, Reward: 0.8, Steps in the episode: 47 Time: 0.2286086082458496\n",
            "Episode: 426, Reward: 0.5, Steps in the episode: 63 Time: 0.299177885055542\n",
            "Episode: 427, Reward: 0.5, Steps in the episode: 40 Time: 0.19421958923339844\n",
            "Episode: 428, Reward: -0.19999999999999996, Steps in the episode: 95 Time: 0.431760311126709\n",
            "Episode: 429, Reward: -0.40000000000000013, Steps in the episode: 122 Time: 0.5717978477478027\n",
            "Episode: 430, Reward: 0.6, Steps in the episode: 40 Time: 0.1923198699951172\n",
            "Episode: 431, Reward: 1.1102230246251565e-16, Steps in the episode: 65 Time: 0.312469482421875\n",
            "Episode: 432, Reward: 1.0, Steps in the episode: 84 Time: 0.4281342029571533\n",
            "Episode: 433, Reward: 0.5, Steps in the episode: 73 Time: 0.3487098217010498\n",
            "Episode: 434, Reward: 0.8, Steps in the episode: 47 Time: 0.23077774047851562\n",
            "Episode: 435, Reward: 0.20000000000000007, Steps in the episode: 171 Time: 0.8245086669921875\n",
            "Episode: 436, Reward: 0.9, Steps in the episode: 79 Time: 0.37008190155029297\n",
            "Episode: 437, Reward: 0.8, Steps in the episode: 56 Time: 0.28614330291748047\n",
            "Episode: 438, Reward: 0.7, Steps in the episode: 41 Time: 0.19284415245056152\n",
            "Episode: 439, Reward: 0.9, Steps in the episode: 35 Time: 0.1653118133544922\n",
            "Episode: 440, Reward: 0.6, Steps in the episode: 111 Time: 0.5176701545715332\n",
            "Episode: 441, Reward: 0.8, Steps in the episode: 61 Time: 0.2887744903564453\n",
            "Episode: 442, Reward: 0.9, Steps in the episode: 84 Time: 0.3931596279144287\n",
            "Episode: 443, Reward: -0.40000000000000013, Steps in the episode: 361 Time: 1.6793041229248047\n",
            "Episode: 444, Reward: -0.09999999999999987, Steps in the episode: 91 Time: 0.41484761238098145\n",
            "Episode: 445, Reward: 0.20000000000000007, Steps in the episode: 188 Time: 1.0547621250152588\n",
            "Episode: 446, Reward: 0.10000000000000009, Steps in the episode: 380 Time: 2.199775457382202\n",
            "Episode: 447, Reward: 0.5, Steps in the episode: 172 Time: 0.7903509140014648\n",
            "Episode: 448, Reward: 0.8, Steps in the episode: 40 Time: 0.19920706748962402\n",
            "Episode: 449, Reward: 0.6, Steps in the episode: 55 Time: 0.2586944103240967\n",
            "Episode: 450, Reward: 0.7, Steps in the episode: 38 Time: 0.17603731155395508\n",
            "Episode: 451, Reward: 0.8, Steps in the episode: 36 Time: 0.17097830772399902\n",
            "Episode: 452, Reward: -0.09999999999999987, Steps in the episode: 167 Time: 0.780235767364502\n",
            "Episode: 453, Reward: 0.9, Steps in the episode: 34 Time: 0.16186213493347168\n",
            "Episode: 454, Reward: 0.30000000000000004, Steps in the episode: 83 Time: 0.40061473846435547\n",
            "Episode: 455, Reward: 0.20000000000000007, Steps in the episode: 68 Time: 0.3350558280944824\n",
            "Episode: 456, Reward: 0.8, Steps in the episode: 188 Time: 0.8922033309936523\n",
            "Episode: 457, Reward: 0.9, Steps in the episode: 77 Time: 0.3667633533477783\n",
            "Episode: 458, Reward: 0.30000000000000004, Steps in the episode: 68 Time: 0.3273344039916992\n",
            "Episode: 459, Reward: 0.6, Steps in the episode: 62 Time: 0.30812907218933105\n",
            "Episode: 460, Reward: 0.8, Steps in the episode: 73 Time: 0.3401758670806885\n",
            "Episode: 461, Reward: 0.4, Steps in the episode: 100 Time: 0.4609196186065674\n",
            "Episode: 462, Reward: 0.7, Steps in the episode: 38 Time: 0.18518471717834473\n",
            "Episode: 463, Reward: 0.9, Steps in the episode: 101 Time: 0.46130800247192383\n",
            "Episode: 464, Reward: -0.7000000000000004, Steps in the episode: 129 Time: 0.605722188949585\n",
            "Episode: 465, Reward: 0.20000000000000007, Steps in the episode: 46 Time: 0.2136857509613037\n",
            "Episode: 466, Reward: -0.30000000000000004, Steps in the episode: 378 Time: 1.6953587532043457\n",
            "Episode: 467, Reward: -0.19999999999999996, Steps in the episode: 283 Time: 1.870241403579712\n",
            "Episode: 468, Reward: 1.1102230246251565e-16, Steps in the episode: 73 Time: 0.45758581161499023\n",
            "Episode: 469, Reward: 0.6, Steps in the episode: 36 Time: 0.16494297981262207\n",
            "Episode: 470, Reward: 0.8, Steps in the episode: 97 Time: 0.4462423324584961\n",
            "Episode: 471, Reward: 0.5, Steps in the episode: 42 Time: 0.18715810775756836\n",
            "Episode: 472, Reward: 0.4, Steps in the episode: 80 Time: 0.39696431159973145\n",
            "Episode: 473, Reward: 0.7, Steps in the episode: 105 Time: 0.48962903022766113\n",
            "Episode: 474, Reward: 0.7, Steps in the episode: 55 Time: 0.26767802238464355\n",
            "Episode: 475, Reward: -0.19999999999999996, Steps in the episode: 81 Time: 0.3661036491394043\n",
            "Episode: 476, Reward: 0.5, Steps in the episode: 72 Time: 0.3349931240081787\n",
            "Episode: 477, Reward: 0.5, Steps in the episode: 97 Time: 0.45551443099975586\n",
            "Episode: 478, Reward: 1.0, Steps in the episode: 81 Time: 0.37851834297180176\n",
            "Episode: 479, Reward: 0.9, Steps in the episode: 41 Time: 0.20114874839782715\n",
            "Episode: 480, Reward: 0.8, Steps in the episode: 39 Time: 0.1879868507385254\n",
            "Episode: 481, Reward: -1.0000000000000004, Steps in the episode: 188 Time: 0.8820610046386719\n",
            "Episode: 482, Reward: 0.7, Steps in the episode: 165 Time: 0.7771828174591064\n",
            "Episode: 483, Reward: 0.7, Steps in the episode: 148 Time: 0.6852562427520752\n",
            "Episode: 484, Reward: 0.8, Steps in the episode: 46 Time: 0.2135934829711914\n",
            "Episode: 485, Reward: 0.9, Steps in the episode: 30 Time: 0.14223790168762207\n",
            "Episode: 486, Reward: 1.0, Steps in the episode: 48 Time: 0.22054243087768555\n",
            "Episode: 487, Reward: 0.30000000000000004, Steps in the episode: 87 Time: 0.40642642974853516\n",
            "Episode: 488, Reward: -0.5000000000000002, Steps in the episode: 199 Time: 0.9166877269744873\n",
            "Episode: 489, Reward: 0.30000000000000004, Steps in the episode: 159 Time: 0.7306404113769531\n",
            "Episode: 490, Reward: 0.9, Steps in the episode: 90 Time: 0.4227943420410156\n",
            "Episode: 491, Reward: 0.9, Steps in the episode: 87 Time: 0.41126489639282227\n",
            "Episode: 492, Reward: 1.1102230246251565e-16, Steps in the episode: 281 Time: 1.8100841045379639\n",
            "Episode: 493, Reward: 0.5, Steps in the episode: 363 Time: 1.8536276817321777\n",
            "Episode: 494, Reward: 0.8, Steps in the episode: 33 Time: 0.15900325775146484\n",
            "Episode: 495, Reward: 1.0, Steps in the episode: 76 Time: 0.36296963691711426\n",
            "Episode: 496, Reward: 0.10000000000000009, Steps in the episode: 130 Time: 0.6278574466705322\n",
            "Episode: 497, Reward: 0.30000000000000004, Steps in the episode: 80 Time: 0.3876047134399414\n",
            "Episode: 498, Reward: -0.5000000000000002, Steps in the episode: 180 Time: 0.8549964427947998\n",
            "Episode: 499, Reward: 1.0, Steps in the episode: 55 Time: 0.2638838291168213\n",
            "Episode: 500, Reward: -0.30000000000000004, Steps in the episode: 220 Time: 1.0524365901947021\n",
            "Episode: 501, Reward: 0.6, Steps in the episode: 47 Time: 0.23447942733764648\n",
            "Episode: 502, Reward: 0.10000000000000009, Steps in the episode: 127 Time: 0.611422061920166\n",
            "Episode: 503, Reward: 0.7, Steps in the episode: 73 Time: 0.3510286808013916\n",
            "Episode: 504, Reward: 0.7, Steps in the episode: 30 Time: 0.16016936302185059\n",
            "Episode: 505, Reward: 0.4, Steps in the episode: 80 Time: 0.3901798725128174\n",
            "Episode: 506, Reward: -0.30000000000000004, Steps in the episode: 315 Time: 1.474869966506958\n",
            "Episode: 507, Reward: 0.30000000000000004, Steps in the episode: 144 Time: 0.6804988384246826\n",
            "Episode: 508, Reward: 0.8, Steps in the episode: 154 Time: 0.7138671875\n",
            "Episode: 509, Reward: -0.09999999999999987, Steps in the episode: 283 Time: 1.8166110515594482\n",
            "Episode: 510, Reward: 0.7, Steps in the episode: 61 Time: 0.4414212703704834\n",
            "Episode: 511, Reward: 0.8, Steps in the episode: 30 Time: 0.16666173934936523\n",
            "Episode: 512, Reward: 0.8, Steps in the episode: 47 Time: 0.22150158882141113\n",
            "Episode: 513, Reward: 0.10000000000000009, Steps in the episode: 57 Time: 0.28635287284851074\n",
            "Episode: 514, Reward: -0.19999999999999996, Steps in the episode: 128 Time: 0.5932669639587402\n",
            "Episode: 515, Reward: 0.20000000000000007, Steps in the episode: 172 Time: 0.8057148456573486\n",
            "Episode: 516, Reward: 0.7, Steps in the episode: 112 Time: 0.5141472816467285\n",
            "Episode: 517, Reward: -0.40000000000000013, Steps in the episode: 112 Time: 0.5365164279937744\n",
            "Episode: 518, Reward: 0.6, Steps in the episode: 151 Time: 0.7266290187835693\n",
            "Episode: 519, Reward: -0.8000000000000005, Steps in the episode: 101 Time: 0.47685766220092773\n",
            "Episode: 520, Reward: 0.5, Steps in the episode: 87 Time: 0.4247105121612549\n",
            "Episode: 521, Reward: 1.1102230246251565e-16, Steps in the episode: 89 Time: 0.42665934562683105\n",
            "Episode: 522, Reward: 0.8, Steps in the episode: 122 Time: 0.5956432819366455\n",
            "Episode: 523, Reward: 0.7, Steps in the episode: 115 Time: 0.5504891872406006\n",
            "Episode: 524, Reward: -0.30000000000000004, Steps in the episode: 126 Time: 0.6233608722686768\n",
            "Episode: 525, Reward: 0.6, Steps in the episode: 65 Time: 0.328568696975708\n",
            "Episode: 526, Reward: 0.10000000000000009, Steps in the episode: 187 Time: 0.8888599872589111\n",
            "Episode: 527, Reward: 0.8, Steps in the episode: 40 Time: 0.1870260238647461\n",
            "Episode: 528, Reward: 0.7, Steps in the episode: 75 Time: 0.36951231956481934\n",
            "Episode: 529, Reward: 0.30000000000000004, Steps in the episode: 182 Time: 0.8527812957763672\n",
            "Episode: 530, Reward: -0.30000000000000004, Steps in the episode: 276 Time: 1.654543399810791\n",
            "Episode: 531, Reward: 0.9, Steps in the episode: 45 Time: 0.3076639175415039\n",
            "Episode: 532, Reward: 0.9, Steps in the episode: 24 Time: 0.1843411922454834\n",
            "Episode: 533, Reward: 0.4, Steps in the episode: 151 Time: 0.8501605987548828\n",
            "Episode: 534, Reward: 0.8, Steps in the episode: 98 Time: 0.4667332172393799\n",
            "Episode: 535, Reward: 1.1102230246251565e-16, Steps in the episode: 73 Time: 0.34653639793395996\n",
            "Episode: 536, Reward: 0.6, Steps in the episode: 220 Time: 1.0363669395446777\n",
            "Episode: 537, Reward: 0.5, Steps in the episode: 69 Time: 0.3200681209564209\n",
            "Episode: 538, Reward: -0.09999999999999987, Steps in the episode: 119 Time: 0.5527534484863281\n",
            "Episode: 539, Reward: -0.30000000000000004, Steps in the episode: 286 Time: 1.329286813735962\n",
            "Episode: 540, Reward: 0.10000000000000009, Steps in the episode: 212 Time: 1.005760669708252\n",
            "Episode: 541, Reward: 0.6, Steps in the episode: 50 Time: 0.2455754280090332\n",
            "Episode: 542, Reward: 1.0, Steps in the episode: 147 Time: 0.6923766136169434\n",
            "Episode: 543, Reward: 0.10000000000000009, Steps in the episode: 140 Time: 0.6847424507141113\n",
            "Episode: 544, Reward: 0.6, Steps in the episode: 129 Time: 0.6241681575775146\n",
            "Episode: 545, Reward: 0.6, Steps in the episode: 81 Time: 0.38762998580932617\n",
            "Episode: 546, Reward: 1.0, Steps in the episode: 52 Time: 0.2493278980255127\n",
            "Episode: 547, Reward: 0.4, Steps in the episode: 58 Time: 0.29891109466552734\n",
            "Episode: 548, Reward: -0.19999999999999996, Steps in the episode: 315 Time: 1.5781662464141846\n",
            "Episode: 549, Reward: 0.7, Steps in the episode: 59 Time: 0.40144896507263184\n",
            "Episode: 550, Reward: 0.5, Steps in the episode: 47 Time: 0.3229866027832031\n",
            "Episode: 551, Reward: 0.5, Steps in the episode: 64 Time: 0.44559335708618164\n",
            "Episode: 552, Reward: 0.10000000000000009, Steps in the episode: 153 Time: 0.9602460861206055\n",
            "Episode: 553, Reward: 0.30000000000000004, Steps in the episode: 124 Time: 0.5849356651306152\n",
            "Episode: 554, Reward: 0.6, Steps in the episode: 69 Time: 0.3400733470916748\n",
            "Episode: 555, Reward: 0.6, Steps in the episode: 127 Time: 0.6399092674255371\n",
            "Episode: 556, Reward: 0.4, Steps in the episode: 135 Time: 0.6358473300933838\n",
            "Episode: 557, Reward: -0.19999999999999996, Steps in the episode: 212 Time: 0.9898192882537842\n",
            "Episode: 558, Reward: 0.4, Steps in the episode: 131 Time: 0.5940055847167969\n",
            "Episode: 559, Reward: -0.09999999999999987, Steps in the episode: 220 Time: 1.00486159324646\n",
            "Episode: 560, Reward: 0.9, Steps in the episode: 71 Time: 0.34840869903564453\n",
            "Episode: 561, Reward: 0.6, Steps in the episode: 95 Time: 0.45528459548950195\n",
            "Episode: 562, Reward: 0.8, Steps in the episode: 72 Time: 0.3472139835357666\n",
            "Episode: 563, Reward: 0.5, Steps in the episode: 190 Time: 0.885490894317627\n",
            "Episode: 564, Reward: 0.20000000000000007, Steps in the episode: 72 Time: 0.3550436496734619\n",
            "Episode: 565, Reward: 0.5, Steps in the episode: 37 Time: 0.17962956428527832\n",
            "Episode: 566, Reward: 1.0, Steps in the episode: 30 Time: 0.14965462684631348\n",
            "Episode: 567, Reward: -0.30000000000000004, Steps in the episode: 205 Time: 0.9602420330047607\n",
            "Episode: 568, Reward: 1.0, Steps in the episode: 155 Time: 0.7210650444030762\n",
            "Episode: 569, Reward: 0.5, Steps in the episode: 206 Time: 1.1132817268371582\n",
            "Episode: 570, Reward: -0.19999999999999996, Steps in the episode: 260 Time: 1.7079126834869385\n",
            "Episode: 571, Reward: 0.6, Steps in the episode: 40 Time: 0.204819917678833\n",
            "Episode: 572, Reward: 0.4, Steps in the episode: 71 Time: 0.34685659408569336\n",
            "Episode: 573, Reward: 0.9, Steps in the episode: 47 Time: 0.24274659156799316\n",
            "Episode: 574, Reward: 0.6, Steps in the episode: 49 Time: 0.23918676376342773\n",
            "Episode: 575, Reward: 0.7, Steps in the episode: 42 Time: 0.21669507026672363\n",
            "Episode: 576, Reward: 0.7, Steps in the episode: 95 Time: 0.450023889541626\n",
            "Episode: 577, Reward: 0.6, Steps in the episode: 190 Time: 0.8974249362945557\n",
            "Episode: 578, Reward: 0.20000000000000007, Steps in the episode: 205 Time: 0.9570827484130859\n",
            "Episode: 579, Reward: 0.7, Steps in the episode: 70 Time: 0.32457590103149414\n",
            "Episode: 580, Reward: 0.9, Steps in the episode: 23 Time: 0.111236572265625\n",
            "Episode: 581, Reward: 0.9, Steps in the episode: 30 Time: 0.14699959754943848\n",
            "Episode: 582, Reward: 0.6, Steps in the episode: 52 Time: 0.25649309158325195\n",
            "Episode: 583, Reward: -0.09999999999999987, Steps in the episode: 135 Time: 0.6098663806915283\n",
            "Episode: 584, Reward: 0.6, Steps in the episode: 52 Time: 0.2633044719696045\n",
            "Episode: 585, Reward: 0.10000000000000009, Steps in the episode: 101 Time: 0.48072028160095215\n",
            "Episode: 586, Reward: 0.20000000000000007, Steps in the episode: 122 Time: 0.5901737213134766\n",
            "Episode: 587, Reward: 1.1102230246251565e-16, Steps in the episode: 242 Time: 1.1549079418182373\n",
            "Episode: 588, Reward: 0.8, Steps in the episode: 157 Time: 0.7262012958526611\n",
            "Episode: 589, Reward: 0.30000000000000004, Steps in the episode: 75 Time: 0.3603639602661133\n",
            "Episode: 590, Reward: 0.6, Steps in the episode: 59 Time: 0.2738935947418213\n",
            "Episode: 591, Reward: 0.9, Steps in the episode: 53 Time: 0.252269983291626\n",
            "Episode: 592, Reward: 0.9, Steps in the episode: 131 Time: 0.6361725330352783\n",
            "Episode: 593, Reward: 0.8, Steps in the episode: 64 Time: 0.4087860584259033\n",
            "Episode: 594, Reward: 0.9, Steps in the episode: 66 Time: 0.45615720748901367\n",
            "Episode: 595, Reward: 0.6, Steps in the episode: 42 Time: 0.2803070545196533\n",
            "Episode: 596, Reward: 1.0, Steps in the episode: 38 Time: 0.2614588737487793\n",
            "Episode: 597, Reward: 0.5, Steps in the episode: 42 Time: 0.30921339988708496\n",
            "Episode: 598, Reward: 0.7, Steps in the episode: 112 Time: 0.6980352401733398\n",
            "Episode: 599, Reward: 0.7, Steps in the episode: 88 Time: 0.413712739944458\n",
            "Episode: 600, Reward: 1.1102230246251565e-16, Steps in the episode: 67 Time: 0.3076024055480957\n",
            "Episode: 601, Reward: 0.10000000000000009, Steps in the episode: 254 Time: 1.152740716934204\n",
            "Episode: 602, Reward: 0.4, Steps in the episode: 92 Time: 0.42662978172302246\n",
            "Episode: 603, Reward: 0.9, Steps in the episode: 68 Time: 0.319307804107666\n",
            "Episode: 604, Reward: -0.8000000000000005, Steps in the episode: 354 Time: 1.6305971145629883\n",
            "Episode: 605, Reward: 0.6, Steps in the episode: 44 Time: 0.2164616584777832\n",
            "Episode: 606, Reward: 0.6, Steps in the episode: 62 Time: 0.28533506393432617\n",
            "Episode: 607, Reward: 1.1102230246251565e-16, Steps in the episode: 156 Time: 0.7531366348266602\n",
            "Episode: 608, Reward: 0.30000000000000004, Steps in the episode: 54 Time: 0.25490498542785645\n",
            "Episode: 609, Reward: 0.20000000000000007, Steps in the episode: 139 Time: 0.6851608753204346\n",
            "Episode: 610, Reward: 0.6, Steps in the episode: 60 Time: 0.2906773090362549\n",
            "Episode: 611, Reward: 0.30000000000000004, Steps in the episode: 204 Time: 1.0113122463226318\n",
            "Episode: 612, Reward: -0.7000000000000004, Steps in the episode: 236 Time: 1.1346778869628906\n",
            "Episode: 613, Reward: -0.30000000000000004, Steps in the episode: 93 Time: 0.4408268928527832\n",
            "Episode: 614, Reward: -0.8000000000000005, Steps in the episode: 272 Time: 1.6174862384796143\n",
            "Episode: 615, Reward: 0.10000000000000009, Steps in the episode: 307 Time: 1.7403311729431152\n",
            "Episode: 616, Reward: 0.7, Steps in the episode: 159 Time: 0.7430543899536133\n",
            "Episode: 617, Reward: 0.5, Steps in the episode: 100 Time: 0.4783337116241455\n",
            "Episode: 618, Reward: 1.1102230246251565e-16, Steps in the episode: 103 Time: 0.4847240447998047\n",
            "Episode: 619, Reward: 0.9, Steps in the episode: 65 Time: 0.3126709461212158\n",
            "Episode: 620, Reward: -0.09999999999999987, Steps in the episode: 191 Time: 0.8904364109039307\n",
            "Episode: 621, Reward: 0.4, Steps in the episode: 82 Time: 0.38733959197998047\n",
            "Episode: 622, Reward: 0.4, Steps in the episode: 142 Time: 0.6586651802062988\n",
            "Episode: 623, Reward: 0.6, Steps in the episode: 88 Time: 0.4211139678955078\n",
            "Episode: 624, Reward: 0.5, Steps in the episode: 70 Time: 0.33768272399902344\n",
            "Episode: 625, Reward: 0.30000000000000004, Steps in the episode: 153 Time: 0.7470109462738037\n",
            "Episode: 626, Reward: -2.2000000000000015, Steps in the episode: 300 Time: 1.47523832321167\n",
            "Episode: 627, Reward: 0.7, Steps in the episode: 76 Time: 0.3794364929199219\n",
            "Episode: 628, Reward: -0.7000000000000004, Steps in the episode: 388 Time: 1.806619644165039\n",
            "Episode: 629, Reward: -3.5999999999999996, Steps in the episode: 633 Time: 3.5884251594543457\n",
            "Episode: 630, Reward: -0.09999999999999987, Steps in the episode: 179 Time: 0.8222384452819824\n",
            "Episode: 631, Reward: 0.6, Steps in the episode: 187 Time: 0.8596301078796387\n",
            "Episode: 632, Reward: 0.10000000000000009, Steps in the episode: 86 Time: 0.3994932174682617\n",
            "Episode: 633, Reward: 0.6, Steps in the episode: 221 Time: 1.039607048034668\n",
            "Episode: 634, Reward: -0.30000000000000004, Steps in the episode: 246 Time: 1.1235442161560059\n",
            "Episode: 635, Reward: 0.6, Steps in the episode: 195 Time: 0.8956007957458496\n",
            "Episode: 636, Reward: 0.30000000000000004, Steps in the episode: 155 Time: 0.7491500377655029\n",
            "Episode: 637, Reward: -0.30000000000000004, Steps in the episode: 241 Time: 1.1633672714233398\n",
            "Episode: 638, Reward: 0.7, Steps in the episode: 131 Time: 0.6248550415039062\n",
            "Episode: 639, Reward: 0.9, Steps in the episode: 175 Time: 0.8198812007904053\n",
            "Episode: 640, Reward: 0.9, Steps in the episode: 32 Time: 0.1829395294189453\n",
            "Episode: 641, Reward: -0.09999999999999987, Steps in the episode: 174 Time: 1.1906070709228516\n",
            "Episode: 642, Reward: -0.30000000000000004, Steps in the episode: 114 Time: 0.7966132164001465\n",
            "Episode: 643, Reward: 0.10000000000000009, Steps in the episode: 219 Time: 1.0551598072052002\n",
            "Episode: 644, Reward: 0.8, Steps in the episode: 60 Time: 0.2936134338378906\n",
            "Episode: 645, Reward: 0.7, Steps in the episode: 54 Time: 0.2502627372741699\n",
            "Episode: 646, Reward: 0.5, Steps in the episode: 82 Time: 0.4020724296569824\n",
            "Episode: 647, Reward: 0.6, Steps in the episode: 85 Time: 0.4356842041015625\n",
            "Episode: 648, Reward: 0.7, Steps in the episode: 86 Time: 0.4148561954498291\n",
            "Episode: 649, Reward: 1.0, Steps in the episode: 61 Time: 0.28733134269714355\n",
            "Episode: 650, Reward: 0.30000000000000004, Steps in the episode: 216 Time: 1.0073139667510986\n",
            "Episode: 651, Reward: 0.6, Steps in the episode: 74 Time: 0.356459379196167\n",
            "Episode: 652, Reward: -0.30000000000000004, Steps in the episode: 146 Time: 0.7061202526092529\n",
            "Episode: 653, Reward: 0.4, Steps in the episode: 80 Time: 0.3765232563018799\n",
            "Episode: 654, Reward: 0.5, Steps in the episode: 186 Time: 0.8692119121551514\n",
            "Episode: 655, Reward: 0.5, Steps in the episode: 110 Time: 0.5135073661804199\n",
            "Episode: 656, Reward: 0.6, Steps in the episode: 129 Time: 0.6155569553375244\n",
            "Episode: 657, Reward: 0.30000000000000004, Steps in the episode: 90 Time: 0.4358639717102051\n",
            "Episode: 658, Reward: -0.9000000000000006, Steps in the episode: 269 Time: 1.2876482009887695\n",
            "Episode: 659, Reward: 0.5, Steps in the episode: 230 Time: 1.2936177253723145\n",
            "Episode: 660, Reward: 0.9, Steps in the episode: 48 Time: 0.32742762565612793\n",
            "Episode: 661, Reward: -0.7000000000000004, Steps in the episode: 148 Time: 1.0366780757904053\n",
            "Episode: 662, Reward: 0.20000000000000007, Steps in the episode: 110 Time: 0.6016476154327393\n",
            "Episode: 663, Reward: -0.7000000000000004, Steps in the episode: 116 Time: 0.5706295967102051\n",
            "Episode: 664, Reward: 0.9, Steps in the episode: 46 Time: 0.22233867645263672\n",
            "Episode: 665, Reward: 0.4, Steps in the episode: 154 Time: 0.7157690525054932\n",
            "Episode: 666, Reward: 0.4, Steps in the episode: 168 Time: 0.7930386066436768\n",
            "Episode: 667, Reward: -0.8000000000000005, Steps in the episode: 121 Time: 0.5702023506164551\n",
            "Episode: 668, Reward: -0.30000000000000004, Steps in the episode: 216 Time: 1.0048949718475342\n",
            "Episode: 669, Reward: 0.5, Steps in the episode: 59 Time: 0.2930471897125244\n",
            "Episode: 670, Reward: -0.30000000000000004, Steps in the episode: 90 Time: 0.43183302879333496\n",
            "Episode: 671, Reward: 0.30000000000000004, Steps in the episode: 159 Time: 0.7588379383087158\n",
            "Episode: 672, Reward: 0.8, Steps in the episode: 44 Time: 0.21242809295654297\n",
            "Episode: 673, Reward: -0.19999999999999996, Steps in the episode: 151 Time: 0.709003210067749\n",
            "Episode: 674, Reward: 0.7, Steps in the episode: 44 Time: 0.20336461067199707\n",
            "Episode: 675, Reward: 0.7, Steps in the episode: 126 Time: 0.5820879936218262\n",
            "Episode: 676, Reward: 0.30000000000000004, Steps in the episode: 157 Time: 0.7446837425231934\n",
            "Episode: 677, Reward: 0.30000000000000004, Steps in the episode: 147 Time: 0.6909654140472412\n",
            "Episode: 678, Reward: 0.7, Steps in the episode: 65 Time: 0.31731724739074707\n",
            "Episode: 679, Reward: 0.7, Steps in the episode: 121 Time: 0.5874843597412109\n",
            "Episode: 680, Reward: 0.9, Steps in the episode: 76 Time: 0.45516085624694824\n",
            "Episode: 681, Reward: 0.9, Steps in the episode: 44 Time: 0.32161641120910645\n",
            "Episode: 682, Reward: 0.6, Steps in the episode: 248 Time: 1.6438195705413818\n",
            "Episode: 683, Reward: 0.10000000000000009, Steps in the episode: 107 Time: 0.4946765899658203\n",
            "Episode: 684, Reward: -0.19999999999999996, Steps in the episode: 201 Time: 0.9370219707489014\n",
            "Episode: 685, Reward: 1.1102230246251565e-16, Steps in the episode: 233 Time: 1.0714247226715088\n",
            "Episode: 686, Reward: 0.6, Steps in the episode: 46 Time: 0.24003958702087402\n",
            "Episode: 687, Reward: 0.10000000000000009, Steps in the episode: 137 Time: 0.6361157894134521\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-2ef71967902f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdql_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_simple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#dql_mid = deep_q_learning(environment_mid, memory, policy_net, target_net, optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dql_hard = deep_q_learning(environment_hard, memory, policy_net, target_net, optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-145-b620db541dbe>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(env, memory, policy_net, target_net, optimizer, alpha, gamma, epsilon, epsilon_decay, target_update_freq, episodes)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;31m#Increment your reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_update_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-fbb001e71a26>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(memory, policy_net, target_net, gamma, optimizer, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dql_training, delta_time, _ = deep_q_learning(environment_simple, memory, policy_net, target_net, optimizer)\n",
        "#dql_mid = deep_q_learning(environment_mid, memory, policy_net, target_net, optimizer)\n",
        "#dql_hard = deep_q_learning(environment_hard, memory, policy_net, target_net, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYGrN8IJKjOw"
      },
      "source": [
        "Initializing empty models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMBaw49FL4yb"
      },
      "outputs": [],
      "source": [
        "policy_net_trained = DeepQNetwork(STATE_SIZE[0]*STATE_SIZE[1], 150, ACTION_SIZE)\n",
        "target_net_trained = DeepQNetwork(STATE_SIZE[0]*STATE_SIZE[1], 150, ACTION_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en8tGc2uKlz5"
      },
      "source": [
        "Loading trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C_jqgxkL9tP"
      },
      "outputs": [],
      "source": [
        "load_weights('policy_net_weights.pth', policy_net_trained)\n",
        "load_weights('target_net_weights.pth', target_net_trained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F69AEnJR85v"
      },
      "source": [
        "---\n",
        "\n",
        "# Spatial Computing for Path Planning - SCPP - Personalized algorithm\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tqdk5DiyG2Y9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ0LGCpfIltd"
      },
      "source": [
        "Checking availability of CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63O6X_KHIlZs",
        "outputId": "cbc7a334-8c39-46f0-a710-d04030b8f5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TluBOVXiG2tn"
      },
      "source": [
        "Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2W0_1r92HzDg"
      },
      "outputs": [],
      "source": [
        "class NeuralAgent(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralAgent, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4hTNDcOIYXD"
      },
      "source": [
        "Function to compute the input tensor during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "jz9K3bMWIX-Z"
      },
      "outputs": [],
      "source": [
        "def calculate_input_tensor(env):\n",
        "    # Get the dimensions of the grid\n",
        "    height, width = env.observation_space\n",
        "\n",
        "    # Get the agent's position\n",
        "    x, y = env.current_pos\n",
        "\n",
        "    # Initialize the input tensor with zeros\n",
        "    input_tensor = np.zeros(6)\n",
        "\n",
        "    # Calculate the distance to the nearest obstacle in the left direction\n",
        "    for i in range(x-1, -1, -1):\n",
        "        if env.grid[y][i] == 1:\n",
        "            input_tensor[0] = x - i\n",
        "            break\n",
        "\n",
        "    # Calculate the distance to the nearest obstacle in the right direction\n",
        "    for i in range(x+1, height):\n",
        "        if env.grid[y][i] == 1:\n",
        "            input_tensor[1] = i - x\n",
        "            break\n",
        "\n",
        "    # Calculate the distance to the nearest obstacle in the up direction\n",
        "    for j in range(y-1, -1, -1):\n",
        "        if env.grid[j][x] == 1:\n",
        "            input_tensor[2] = y - j\n",
        "            break\n",
        "\n",
        "    # Calculate the distance to the nearest obstacle in the down direction\n",
        "    for j in range(y+1, width):\n",
        "        if env.grid[j][x] == 1:\n",
        "            input_tensor[3] = j - y\n",
        "            break\n",
        "\n",
        "    # Calculate the distance to the goal in the horizontal direction\n",
        "    # Check if the goal exists in the grid before accessing its position\n",
        "    goal_indices = np.argwhere(env.grid == 2)\n",
        "    if goal_indices.size > 0:  # Check if goal_indices is not empty\n",
        "        goal_position = goal_indices[0]  # Access the first goal position if it exists\n",
        "        input_tensor[4] = goal_position[1] - y\n",
        "        input_tensor[5] = goal_position[0] - x\n",
        "    else:\n",
        "        # Handle the case where the goal is not found (e.g., set distances to a default value)\n",
        "        input_tensor[4] = 0  # Or some other appropriate default\n",
        "        input_tensor[5] = 0  # Or some other appropriate default\n",
        "\n",
        "    # Convert the input tensor to a PyTorch tensor\n",
        "    input_tensor = torch.tensor(input_tensor, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMmkS4qOme5a"
      },
      "source": [
        "Epsilon-greedy for SCPP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "li0QZF5Pmghq"
      },
      "outputs": [],
      "source": [
        "def compute_action_scpp_torch(environment, epsilon, agent):\n",
        "\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "        return np.random.choice(range(4)) # Exploration\n",
        "\n",
        "    else:\n",
        "        probs = agent(environment)\n",
        "        return torch.argmax(probs).item() # Exploit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1CH12riHz1x"
      },
      "source": [
        "Definition of the training\n",
        "\n",
        "# DOIT IMPLEMENTER EPSILON-GREEDY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "KSyym_qEH3w0"
      },
      "outputs": [],
      "source": [
        "def scpp(agent, env, num_episodes, criterion, optimizer, epsilon = 1, epsilon_decay = 0.003):\n",
        "    # Train the agent for a specified number of episodes\n",
        "\n",
        "    start_time = time.time()\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        # Initialize the episode reward\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        # Loop through the episode\n",
        "        while True:\n",
        "            # Convert the state to a PyTorch tensor\n",
        "            state_tensor = calculate_input_tensor(env)\n",
        "\n",
        "            # Forward pass\n",
        "            action = compute_action_scpp_torch(state_tensor, epsilon, agent)\n",
        "\n",
        "            # Sample an action from the action probabilities\n",
        "           # action = torch.multinomial(action_probs, num_samples=1).item()\n",
        "\n",
        "\n",
        "            # Take the action and observe the next state and reward\n",
        "            posp1, next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Update the episode reward\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Convert the next state to a PyTorch tensor\n",
        "            next_state_tensor = calculate_input_tensor(env)\n",
        "\n",
        "            # Calculate the target tensor\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target = agent(next_state_tensor)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(target, torch.tensor([action]))\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the state\n",
        "            state = next_state\n",
        "            if steps > 1000:\n",
        "              done = True\n",
        "            # Check if the episode is done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            steps +=1\n",
        "\n",
        "        rewards.append(episode_reward)\n",
        "\n",
        "        # Print the episode reward every 100 episodes\n",
        "       # if (episode + 1) % 100 == 0:\n",
        "            #print(f'Episode [{episode+1}/{num_episodes}], Episode Reward: {episode_reward:.2f}')\n",
        "        print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
        "\n",
        "    save_weights('scpp_weights_simple.pth', policy_net)\n",
        "\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ktup2TcM8I"
      },
      "source": [
        "Initializing an agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "vhTGxpptcQX9"
      },
      "outputs": [],
      "source": [
        "scpp_agent = NeuralAgent(6, 128, 4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(scpp_agent.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmOvgPiIccKG"
      },
      "source": [
        "Running of the training of the SCPP Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "pWk18xD3ceBR",
        "outputId": "3594ba67-e2b2-403d-8ee4-05231731a99b"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-988523e988f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscpp_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment_simple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-121-43580ecdfb08>\u001b[0m in \u001b[0;36mscpp\u001b[0;34m(agent, env, num_episodes, criterion, optimizer, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "scpp(scpp_agent, environment_simple, 1000, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4tnRQokceY3"
      },
      "source": [
        "Loading of empty agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVt69BtychpS"
      },
      "source": [
        "Loading of trained weights"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
